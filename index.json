[{"categories":["Others"],"contents":"In today\u0026rsquo;s fast-paced digital landscape, integrating various cloud services into small projects has become crucial for streamlined and efficient project management. This article delves into the world of Amazon Web Services (AWS), providing a comprehensive guide on how to harness its capabilities for your projects. We will explore how to set up and manage key AWS services, including IAM for user and policy configuration, VPC for network setup, RDS for database management, EC2 for server instances, and S3 for storage solutions.\nTable of Contents  Table of Contents 1. Setting Up IAM Users Policies, and User Group: 2. Configuring VPC and Subnets: 3. Security Group and RDS Configuration: 4. EC2 Instance Configuration: 5. S3 Storage Configuration and Usage: 6. Frontend and backend Integration: 7. Pausing and Resuming Projects: 8. Resource Cleanup: 9. Data Backup:  1. Setting Up IAM Users Policies, and User Group: In the world of AWS, security is paramount. This step involves creating new IAM policies, granting specific access permissions to services like EC2, VPC, RDS, and S3. It\u0026rsquo;s essential to configure roles and permissions related to EC2 and RDS, focusing on capabilities like ListRole, CreateRole, PassRole, and ListInstanceProfiles. To effectively demonstrate a demo, we\u0026rsquo;ll create two IAM users: \u0026lsquo;bruce-demo\u0026rsquo; and \u0026lsquo;kuang-demo\u0026rsquo;. These users will be assigned appropriate policy permissions, serving as practical examples in our demonstration.\nPolicies: User Group: 2. Configuring VPC and Subnets: The creation of a VPC, dubbed \u0026lsquo;aws-demo-vpc\u0026rsquo; with an IP range of 10.0.0.0/16, lays the foundation of your network infrastructure in AWS. Subnet configuration, including the setup of public and private subnets, plays a significant role. Establishing an Internet Gateway (IGW) for the public subnet and configuring the Route Table ensures seamless internet connectivity.\nVPC: Subnets: IGW: 3. Security Group and RDS Configuration: Security is not just a feature but a necessity. Setting up a security group named \u0026lsquo;aws-demo-review-rds-sg\u0026rsquo; with appropriate inbound and outbound rules facilitates the connection to a MySQL database. The creation of a Subnet Group and a database instance named \u0026lsquo;aws-demo-rds\u0026rsquo; in RDS, with MySQL as the database engine, enhances the security posture. Adjusting settings like IOPS, storage auto-scaling, and Multi-AZ configurations ensures a robust and resilient database setup.\nSecurity Group: RDS_SG: RDS: 4. EC2 Instance Configuration: Launching EC2 instances is a crucial aspect of this setup. We initiate two instances, both derived from the Amazon Linux AMI. These are placed within the public subnet and assigned public IP addresses to ensure network accessibility. To tailor their accessibility, we configure security groups, setting up distinct ports for each: port 8080 for the first instance (EC2-1), designated for front-end simulation, and port 3000 for the second instance (EC2-2), intended for back-end simulation.\nThe functionality of these instances is further enhanced by installing Docker and docker-compose, which aid in the deployment and management of applications, along with GIT for version control. This arrangement effectively creates a practical environment, where one VM (EC2-1) acts as a front-end server and the other (EC2-2) as a back-end server, demonstrating a typical web application infrastructure.\n5. S3 Storage Configuration and Usage: The configuration of S3 storage is vital for managing resources effectively. In EC2-1, creating an S3 Bucket for storing frontend resources, such as cover images, and implementing version control to prevent data loss demonstrates a well-thought-out storage strategy. Configuring IAM roles for EC2 instances to interact with S3 services further integrates your storage solutions.\nS3: 6. Frontend and backend Integration: The photograph provided illustrates a successful implementation using AWS services, particularly with EC2 instances and RDS.\nIn this setup, EC2-2 (backend) plays a crucial role in connecting to the RDS (Relational Database Service) to retrieve data. This data management is fundamental, as RDS offers scalable and efficient database storage, essential for dynamic web applications.\nMeanwhile, EC2-1 (frontend) is configured to fetch database data through the backend API. This architecture allows the front-end of the application to display data on the webpage dynamically and interactively. The separation of concerns between the frontend and backend ensures a more secure, efficient, and manageable system.\nAdditionally, the use of Amazon S3 (Simple Storage Service) is highlighted for image hosting. By storing images in S3 and retrieving their public OBJECT URLs, the application can easily embed these images on the web pages. This approach not only simplifies the management of web resources but also leverages the high availability and durability of AWS S3.\nFurthermore, the interface includes a dialog box where users can insert new data into the database directly from the webpage. This feature enhances user interaction and data input efficiency.\nEC2-1 (frontend): EC2-2 (backend): 7. Pausing and Resuming Projects: The ability to pause and resume resources efficiently is crucial. Stopping RDS and EC2 resources, creating snapshots and AMIs, and restoring databases and instances from these snapshots and AMIs are essential for maintaining service continuity. Updating connection parameters and restarting services ensures minimal downtime and efficient resource management.\nRDS-Snapshot: 8. Resource Cleanup: Finally, effective resource management involves cleaning up after project completion. Deleting RDS instances, snapshots, and subnet groups; terminating EC2 instances and deleting related AMIs and snapshots; removing S3 buckets; and cleaning up the VPC and associated resources are critical steps to avoid unnecessary costs and maintain a clutter-free AWS environment.\n9. Data Backup: Regularly creating snapshots or AMIs of RDS and EC2 ensures that you are prepared for disaster recovery. This proactive approach to data backup is key to maintaining data integrity and business continuity in the face of unforeseen challenges.\nThis guide offers a structured approach to setting up and managing your AWS environment, ensuring that each component is configured for optimal performance and security. By following these steps, you can establish a robust and scalable AWS infrastructure tailored to your specific project needs.\n","permalink":"https://bruceewue.github.io/blog/aws_project/","tags":["AWS","Project"],"title":"Project - Building a small project with AWS"},{"categories":["Data Engineering"],"contents":"When it comes to working with data, many challenges arise. One of the biggest challenges is building a reliable and scalable data pipeline that can handle large volumes of data while ensuring the data\u0026rsquo;s accuracy and consistency. With dbt, you can transform, test, and document your data in a reproducible and scalable way. In this article, I\u0026rsquo;ll introduce dbt and show you how to utilise it with BigQuery.\nProject Diagram Source: DE_zoomcamp\nChallenges In the past, data engineering and analytics were distinct fields with unique tools, languages, and skill sets. Data pipelines that are operated on platforms like Apache Spark or Amazon Redshift are orchestrated by data engineers using tools like Apache Airflow or AWS Glue. To query and alter data that is stored in databases or data warehouses, data analysts utilise technologies like Python or SQL.\nHowever, this separation has some drawbacks:\n It separates data teams into silos, which makes it difficult for them to collaborate and align. Managing numerous tools and platforms adds complexity and overhead. It reduces agility and speed in delivering data products and insights.  To address these challenges, a new paradigm has emerged: Analytics Engineering.\nAnalytics engineering applies software engineering principles and techniques to data transformation and analysis. Analytics engineering aims to bridge the gap between data engineering and analytics by enabling data teams to work directly within the data warehouse using a single tool: dbt.\nWhat is dbt (data build tool)? Source: dbt\ndbt is an open-source command-line tool that enables data analysts and engineers to transform raw data into valuable insights. It assists users in managing their entire data pipeline, from extracting data from sources to transforming and loading it into a target data warehouse. With dbt, users can build, test, and deploy data pipelines as code, making it easier to maintain and scale their data infrastructure.\ndbt is designed to work with SQL-based data warehouses, such as BigQuery, Redshift, Snowflake, and others. It uses a declarative syntax to define data transformations, making it easy for non-technical stakeholders to understand what the data is doing. dbt also has a testing framework that enables users to ensure data quality and consistency throughout the pipeline.\nSource: dbt\nWhat are the features of dbt? Modularity: dbt allows you to break down your data pipeline into smaller, reusable components, which can make it easier to maintain and scale over time.\nVersioning: dbt uses Git to manage versions of your code, which makes it easy to track changes and collaborate with others.\nTesting: dbt provides a testing framework that allows you to test your data pipeline at different stages of the pipeline, ensuring the accuracy and consistency of your data.\nDocumentation: Documentation is generated automatically from our models, sources,and tests using Markdown syntax. Documentation provides metadata such as descriptions, tags, columns types, etc.,as well as lineage diagrams that show how datasets depend on each other.\nSeeds: Seeds are CSV files that contain static or reference data such as lookup tables or configuration parameters. Seeds allow us to load small amounts of data into the warehouse without using external sources.\nPackages: Packages are collections of models,sources,tests,macros etc.,that can be shared across projects or with the community via GitHub or other platforms. Packages allow us to leverage existing code or best practices from other users or organizations.\nUsing dbt Cloud dbt Cloud is a web-based application that helps data teams configure, deploy, and control dbt project. dbt Cloud leverages the power of dbt Core and adds features such as:\nIntegrated development environment (IDE): A web-based interface that allows you to write, test, and run dbt models with syntax highlighting, autocomplete, error checking, and version control integration.\nScheduling: A feature that lets you schedule dbt jobs to run automatically at specified intervals or triggers. You can also monitor the status of your jobs and get alerts for failures or anomalies.\nDocumentation: A feature that generates interactive documentation for your dbt project based on your code comments and metadata. You can explore your data models, view lineage graphs, query results, tests, sources.\nMonitoring \u0026amp; alerting: A feature that helps you track the performance and quality of your data models with metrics such as run duration, freshness, row count, test results etc.\nGet started with dbt cloud First, we must create a dbt cloud account and set up the project by connecting to my data warehouse(BigQuery) and Github. Then, click \u0026ldquo;initialize your project\u0026rdquo; on dbt Cloud Here, we\u0026rsquo;ll be able to configure your project settings(dbt_project.yml).\nOnce you\u0026rsquo;ve set up your project settings, you can build and deploy your data pipeline using dbt Cloud. We\u0026rsquo;ll need to create a series of data models in your dbt project to do this.\nTo create a new data model, simply create a new SQL file in your dbt project directory and define your data model using SQL. You can also define your data model using YAML files, which define the dependencies between your models.\nOnce you\u0026rsquo;ve defined your data models, you can use command dbt run to compile your data models into SQL queries and applies any transformations to your data. It checks for any changes in your source data and creates the final output. Essentially, it runs your data pipeline end-to-end.\ndbt lineage graph is a visual representation of the dependencies and relationships between data sources, models, and outputs in a dbt project1. It is based on the concept of a directed acyclic graph (DAG), which is a way of modeling data transformations as a series of nodes and edges1. A dbt lineage graph can help you understand how your data flows from raw sources to final reports, identify potential issues or inefficiencies in your project structure, and document your data pipeline for other stakeholders.\nDeployment To deploy your dbt project using dbt cloud, we need to follow these steps\n Create a deployment environment by selecting “Deploy” and then “Environments”. Name your environment, add a target dataset, and select an account type. Schedule a job by selecting “Deploy” and then “Jobs”. Name your job, select an environment, choose a run frequency, and add any custom commands or notifications. Deploy your project by clicking on the “Deploy” button in the dbt cloud UI. You can also trigger a manual run by clicking on the “Run” button.  Here we can see the document:\nData studio Finally, we can use Google Data Studio to connect BigQuery and present the transformed data through the dashboard.\nConclusion dbt is a powerful tool that helps data teams manage complex data pipelines more efficiently. By offering a user-friendly online interface, automated deployment tools, and enterprise-level security features, dbt Cloud expands the capability of dbt. With dbt and dbt Cloud, data teams can collaborate more effectively, deploy changes faster, and ensure data quality with confidence. If you are looking for a fast and reliable way to transform your data warehouse using SQL, then you should give dbt Cloud a try. It will help you streamline your workflow, improve collaboration,and deliver trusted insights faster.\nAdditional Resources  DE-zoomcamp_week4_analytics_engineering  ","permalink":"https://bruceewue.github.io/blog/de_dbt/","tags":["GCP","BigQuery","dbt","Data Studio","dezoomcamp"],"title":"Analytics Engineering (dbt + BigQuery + Data Studio)"},{"categories":["Machine Learning"],"contents":"In this article, I\u0026rsquo;ll explore how to deploy an existing Keras model to AWS-EKS using TensorFlow Serving and Kubernetes. TensorFlow Serving is a flexible and high-performance system for serving machine learning models, and Kubernetes is an open-source container orchestration system that automates the deployment, scaling, and management of containerized applications.\nTo deploy a Keras model to AWS-EKS, we will first need to package the model using TensorFlow Serving. We will then create a Kubernetes deployment that includes a container running the TensorFlow Serving image, which will serve our model. The diagram below illustrates the architecture of the deployment, with the yellow circles indicating the components that will be deployed to Kubernetes.\nIn this project, we have two key components:\n  Gateway, which downloads images, resizes them, and turns them into a numpy array. This is computationally inexpensive and can be done using a CPU.\n  Model, which involves matrix multiplications that are computationally expensive. Therefore, we might need to use GPUs to handle this component.\n  The advantage of this setup is that the two components can be scaled independently. For example, we may need five gateway instances but only two instances of TF Serving with high-performance GPUs to handle the computationally expensive model component.\nTensorFlow Serving TensorFlow Serving is an open-source platform for serving machine learning models. It provides a flexible, high-performance serving system for deploying TensorFlow models in production environments. With TensorFlow Serving, you can quickly deploy new models and run experiments while maintaining the same server architecture and APIs. It provides a gRPC interface for clients to send prediction requests and receive responses.\nFirst, convert the existing Keras model.\n# convert.py import tensorflow as tf from tensorflow import keras model = keras.models.load_model(\u0026#39;./clothing-model.h5\u0026#39;) # Convert the model to TF format tf.saved_model.save(model,\u0026#39;clothing-model\u0026#39;) Containerization Next, we need to containerize the model and gateway, and create separate Docker images for each. To enable communication between the gateway and model containers, we can use docker-compose.\n#image-model.dockerfileFROMtensorflow/serving:2.7.0COPY clothing-model /models/clothing-model/1ENV MODEL_NAME=\u0026#34;clothing-model\u0026#34;#image-gateway.dockerfileFROMpython:3.8.12-slimRUN pip install pipenvWORKDIR/appCOPY [\u0026#34;Pipfile\u0026#34;, \u0026#34;Pipfile.lock\u0026#34;, \u0026#34;./\u0026#34;]RUN pipenv install --system --deployCOPY [\u0026#34;gateway.py\u0026#34;, \u0026#34;proto.py\u0026#34;, \u0026#34;./\u0026#34;]EXPOSE9696ENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;--bind=0.0.0.0:9696\u0026#34;, \u0026#34;gateway:app\u0026#34;]Kubernetes (K8s) Source: kubernetes.io\nKubernetes is an open-source platform for automating the deployment, scaling, and management of containerized applications. It provides a unified API for deploying and managing containers, and it can run on a variety of cloud platforms and on-premise servers. With Kubernetes, you can automate the deployment and scaling of your applications, and ensure that your services are always available and up-to-date.\nHere are the key components and features of Kubernetes:\n  Nodes: Nodes are the worker machines in a Kubernetes cluster, where the applications and services are deployed. Nodes can be physical machines or virtual machines running in a cloud environment.\n  Pods: Pods are the smallest and simplest unit in the Kubernetes object model. They contain one or more containers, and all containers in a pod share the same network namespace, making it possible for them to communicate with each other using localhost. The pod runs on a node.\n  Services: The entry point of an application and route requests to pods. Services are abstractions that define a set of pods and provide a stable IP address and DNS name to access them. Services also provide load balancing and support for rolling updates, making it easy to deploy new versions of an application.\n  Deployments: A Deployment is a higher-level object in Kubernetes that manages a set of replicas of a pod. The Deployment provides a declarative way to manage the desired state of a set of pods, and ensures that the desired number of replicas are always running and available.\n  Horizontal Pod Autoscaling (HPA) is a Kubernetes feature that allows you to automatically scale the number of replicas of a pod based on CPU utilization, memory utilization, or custom metrics. This helps to ensure that your application can handle varying levels of traffic and demand without manual intervention.\n  In Kubernetes, an Ingress is an API object which is a powerful and flexible way to manage external HTTP(S) traffic to your internal services in the cluster, allowing you to easily expose and route traffic to your services while maintaining control over security and routing policies. (External = Load Balancer, Internal = Cluster IP)\n  Before we start, we need to install Kubectl and Kind.\n  kubectl: kubectl is the command-line tool for interacting with a Kubernetes cluster. With kubectl, you can perform a variety of tasks, such as creating and managing resources, accessing logs and status information, and debugging applications. kubectl is a versatile and powerful tool that is an essential part of working with Kubernetes.\n  kind: kind is a tool for running Kubernetes clusters on local development machines. kind makes it easy to spin up a fully-functional Kubernetes cluster on your own machine, which is useful for testing and development purposes. With kind, you can experiment with different configurations, test new features, and debug issues without having to rely on a remote cluster.\u0026rsquo;\n  Deploying TensorFlow Models to Kubernetes -Model-\n# load a Docker image into a Kind cluster. $ kind load docker-image zoomcamp-10-model:xception-v4-001 # list the pods in a Kubernetes cluster. $ kubectl get pod $ kubectl apply -f kube-config/model-deployment.yaml $ kubectl port-forward tf-serving-clothing-model-548c6-fd5n8 8500:8500 # apply a configuration file to a Kubernetes cluster $ kubectl apply -f kube-config/model-service.yaml # forward traffic from a local machine to a service in a Kubernetes cluster $ kubectl port-forward service/tf-serving-clothing-model 8500:8500 -Gateway-\n$ kind load docker-image zoomcamp-10-gateway:001 $ kubectl apply -f kube-config/gateway-deployment.yaml $ kubectl port-forward gateway-7d467-r8dkc 9696:9696 # need to add \u0026#39;type: LoadBalancer\u0026#39; in gateway-service.yaml $ kubectl apply -f kube-config/gateway-service.yaml $ kubectl port-forward service/gateway 8080:80 Deploying to EKS # eks-config.yamlapiVersion:eksctl.io/v1alpha5kind:ClusterConfigmetadata:name:mlzoomcamp-eksregion:eu-west-1nodeGroups:- name:ng-m5-xlargeinstanceType:m5.xlargedesiredCapacity:1# create an Amazon EKS cluster $ eksctl create cluster -f eks-config.yaml # create an Elastic Container Registry (ECR) repository for doecker images $ aws ecr create-repository --repository-name mlzoomcamp-images 用 After running this line of code, you will see the \u0026ldquo;repositoryUri\u0026rdquo;. Send a request through the following code.\n# testing.py import requests url=\u0026#39;4290xxxxx.ap-northeast-1.elb.amazonaws.com/predict/\u0026#39; data = {\u0026#39;url\u0026#39;,\u0026#39;http://bit.ly/mlbookcamp-pants\u0026#39;} result=requests.post(url,json=data).json() print(result) We can see the newly added cluster in EKS; besides, you can also find the added NODE in EC2, as well as the Load Balancer.\nAdditional Resources  mlbookcamps-kubernetes  ","permalink":"https://bruceewue.github.io/blog/ml_k8s_eks/","tags":["AWS","Kubernetes"],"title":"Model Deployment (K8s and EKS)"},{"categories":["Data Engineering"],"contents":"This post demonstrates how to scrape live bitcoin price data using these tools: Airflow, MongoDB, and Kafka. The data is then saved to a NoSQL database (MongoDB) and periodically processed (ETL) to change the price unit from USD to GBP.\nProject Diagram Environment Ubuntu 20.04 (WSL2) + python 3.8\nData source Source: coincap\nUtilize its API(coincap_api) to scrape real-time (live) bitcoin data.\nMongoDB NoSQL(aka \u0026ldquo;not only SQL\u0026rdquo;), which is different from relational databases, is used here due to the necessity to process high-speed and mass-produced data. NoSQL databases are designed for unstructured data and ideal for big data applications(schema-free).\nSource:MongoDB\nMapping relational database to MongoDB Source:RDBMS_MongoDB\nMongoDB stores data in BSON format(Binary JSON)， \u0026ldquo;_id\u0026rdquo; is an OBJECT not STRING. To prevent errors when extracting data from the database, we can convert it to STRING.\nWe can use Studio3T (free MongoDB GUI) to connect to our MongoDB database.\nUse python to connect to mongoDB import pymongo client = pymongo.MongoClient(\u0026#39;localhost\u0026#39;, 27017) #load data from mongoDB to Pandas database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name query = {} records = pd.DataFrame(list(collection.find(query))) # insert data into mongoDB df = pd.DataFrame({\u0026#39;price\u0026#39;: [\u0026#39;12.4\u0026#39;, \u0026#39;1.4\u0026#39;, \u0026#39;2.6\u0026#39;]}) records = df.to_dict(\u0026#39;records\u0026#39;) collection.insert_many(records) Kafka Apache Kafka is a popular open-source distributed event streaming platform designed to handle continuous streams of events in real-time. Because of the bottlenecks that might be caused when large amounts of data continue to be stored directly in the DB, we can consider using Kafka here.\nSource:Kafka\nThree steps to implement for event streaming:\n To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. To store streams of events durably and reliably for as long as you want. To process streams of events as they occur or retrospectively.  First, we need to start both Zookeper and Kafka services.\nbin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties Open a terminal, create a new topic named \u0026ldquo;kafka-topics\u0026rdquo;\nbin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1  Producers: client applications that publish (write) events to Kafka consumers: those that subscribe to (read and process) these events  # producer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; producer = KafkaProducer(bootstrap_servers=[brokers]) url = \u0026#34;https://api.coincap.io/v2/assets/bitcoin\u0026#34; while True: r = requests.get(url) v = json.dumps(r.json()).encode() k = datetime.now().strftime(\u0026#34;%Y-%m-%d%H:%M:%S:%f\u0026#34;).encode() producer.send(topic, key=k, value=v) # time.sleep(5) The following section is consumer, which receives the data and stores it in the DB\n# consumer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; consumer = KafkaConsumer(topic, bootstrap_servers=[brokers]) client = pymongo.MongoClient(\u0026#34;localhost\u0026#34;, 27017) database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name for msg in consumer: dict_data = json.loads(msg.value) dict_data[\u0026#34;data\u0026#34;][\u0026#34;timestamp\u0026#34;] = msg.key.decode() collection.insert_many([dict_data[\u0026#34;data\u0026#34;]]) Airflow Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. I\u0026rsquo;d like to automatically convert the price of bitcoin from USD to GBP and then store them in the DB.\nBefore using Airflow, we need to set up the connection to the DB(MongoDB) via the Airflow Web UI. (default: pymongo)\nWe need to add the folder \u0026ldquo;dags\u0026rdquo; under the airflow folder and then add a python file (.py) in the folder to define the three tasks of ETL in the DAG, namely Extract, Transform, and Load. They are expected to be executed sequentially at 00:30 every day. Here\u0026rsquo;s a quick, simple, and free editor for cron schedule crontab.guru, which can help us set the schedule_interval.\n dags/dag.py   task1 -\u0026gt; task2 -\u0026gt; task3 Log Result Additional Resources  Kafka Airflow  ","permalink":"https://bruceewue.github.io/blog/etl/","tags":["Airflow","Kafka","MongoDB"],"title":"Data pipeline (cryptocurrency)"},{"categories":["Machine Learning"],"contents":"Greater model parameters and network design are always preferred for better performance during model training. Manually tuning these parameters and architecture can be difficult and time-consuming, so in this post, I\u0026rsquo;ll attempt to use an AutoML tool called Autokeras to accelerate this process.\nAutoKeras AutoKeras is an open-source AutoML tool based on Keras that automates designing and tuning machine learning models. It allows users to quickly and easily train high-performance models without extensive expertise in machine learning. By using Autokeras, we can save time and effort compared to manual tuning and train high-quality models for a variety of tasks.\nAutoKeras uses Efficient Neural Architecture Search (ENAS) to automate the process of designing and tuning machine learning models. It also includes support for a variety of preprocessing methods, such as normalization, vectorization, and encoding. AutoKeras can be used for a wide range of tasks, including structured data, image data, time series prediction, and multitasking.\nSource: AutoKeras\nThe official website of AutoKeras provides examples that show how to use the tool in just a few lines of code.\nimport autokeras as ak # Initialize the image classifier. clf = ak.ImageClassifier() # Feed the image classifier with training data. clf.fit(x_train, y_train) # Predict with the trained model. predicted_y = clf.predict(x_test) Environment With Colab Pro, some Python modules and GPU are already installed and configured in Colab so that we can focus on coding.\nExample 1 - Fraud detection The first case is a credit card fraud detection on Kaggle, which is a structured data classification problem\ndataset: Kaggle-Data-Credit-Card-Fraud-Detection (284807 rows × 31 columns)\nThe StructuredDataClassifier is used here to handle the structured data classification problem.\n# StructuredDataClassifier cbs = [ # Switch to the next model after three consecutive training cycles without improvement # monitor default: val_loss tf.keras.callbacks.EarlyStopping(patience=3,monitor=\u0026#39;val_loss\u0026#39;), # histogram_freq=1 means to count once in each training cycle, and display the weight and bias distribution of each layer of neurons during training in a histogram tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1) ] # max_trials is the number of models to try, the default value is 100, you can find the structure of the default model in oracle.json in the directory clf=ak.StructuredDataClassifier(max_trials=3) # The default value of epoch is 1000, and it may end early when using earlystopping history=clf.fit(x_train,y_train,callbacks=cbs,epochs=25) In addition to directly using StructuredDataClassifier, if you need to customize the model architecture, search space, etc., you can also use AutoModel to further design the model, as shown below.\ninput_node = ak.StructuredDataInput() output_node = ak.StructuredDataBlock()(input_node) output_node = ak.ClassificationHead()(output_node) clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3) Save and load models:\n# Save model try: # TensorFlow SavedModel format model.save(\u0026#39;model_autokeras\u0026#39;,save_format=\u0026#39;tf\u0026#39;) except: # keras h5 format model.save(\u0026#39;model_autokeras.h5\u0026#39;) # Load AutoKeras model from tensorflow.keras.models import load_model loaded_model = load_model(\u0026#39;./auto_model/best_model\u0026#39;, custom_objects=ak.CUSTOM_OBJECTS) Training process (Colab)  If the training is interrupted and executed again, it will continue the training based on the records After the training is completed, AutoKeras will use all training set + validation set for training again with the best model. After all trials are completed, the model with the best performance will be automatically stored in best_model/* Due to the problem of category imbalance in this data set, Autokeras also supports setting class_weight in fit() for weighting the loss function, so that the model will pay more attention to minority categories.  Training Visualization - TensorBoard To understand the training process, you can use tensorboard to read the log data stored in callback to visualize the training process; use the tensorboard.dev to generate a public page for sharing with others.\nimport tensorflow as tf import datetime from tensorboard.plugins.hparams import api as hp !tensorboard dev upload --logdir logs \\ --name \u0026#34;Simple experiment\u0026#34; \\ --description \u0026#34;fraud detect\u0026#34; \\ Execute the above program fragment and check \u0026ldquo;Reload data\u0026rdquo; before training, and the data will be updated during training.\nAccuracy and loss changes during training Model architecture ClearML If you want to monitor and track the results and share the experiments with people, you can also use the MLOps platform ClearML that supports AutoKeras to present it on an interactive web page.\nfrom clearML import Task # register at https://app.community.clear.ml # get the \u0026#39;key\u0026#39; and \u0026#39;secret\u0026#39;： Task.set_credentials( api_host=\u0026#39;https://api.community.clear.ml\u0026#39;, web_host=\u0026#39;https://app.community.clear.ml\u0026#39;, files_host=\u0026#39;https://files.community.clear.ml\u0026#39;, key=\u0026#39;xxxxx\u0026#39;, secret=\u0026#39;xxxxxx\u0026#39; ) # Create a new ClearML task # Visit https://app.community.clear.ml/dashboard task = Task.init(project_name=\u0026#39;autokeras age\u0026#39;, task_name=\u0026#39;age regressor\u0026#39;) The project overview is on the ClearML main page In addition to the loss and accuracy during training, it also includes the machine\u0026rsquo;s status. Changes in weight and bias in the model  ClearML will also show the results through the library matplotlib and seaborn  Example 2 - Age prediction This example is about age prediction, which is a regression problem to predict age from photos.\n(Dataset: wiki_crop)\nAfter image filtering and pre-processing, the images were converted to 52984 128*128 images.\nTraining data: Use AutoModel to customize model details, which is similar to Keras functional API. AutoKeras then finds the best result from the conditions we give.\ninput_node = ak.ImageInput() # If just want to try EfficientNet b0~b7 output_node = ak.ImageBlock(block_type=\u0026#39;efficient\u0026#39;, normalize=True, augment=True)(input_node) output_node = ak.RegressionHead(droupout=0.25)(output_node) reg = ak.AutoModel(inputs=input_node,outputs=output_node, max_trials=3) Testing results after a few epochs of simple training AutoKeras also supports dealing with Multi-modal data and multi-task models, using AutoModel to customize complex model architecture. For example, the figure below contains two types of input data, and finally, the classification label and regression value have to be predicted at the same time.\nSource: Multi-Modal and Multi-Task\nAdditional Resources  AutoKeras Getting started with TensorBoard.dev AutoML 自動化機器學習  ","permalink":"https://bruceewue.github.io/blog/autokeras/","tags":["AutoML"],"title":"AutoKeras (AutoML)"},{"categories":["Data Engineering"],"contents":"Since I\u0026rsquo;d like to know the situation of local data-related vacancies in London, England, I have built a job vacancy analysis system, including a distributed crawler with automatic scheduling, a job vacancy analysis dashboard, and a data API. The results are shown at the end of this post.\nProject Diagram Environment Development: Ubuntu 20.04(WSL2) + python 3.9\nDeployment: VM(2 OCPUs(ARM64), 12GB RAM) x 2 in Oracle Cloud\nData source Indeed job search: Source: Indeed\nCurrently, I choose the UK\u0026rsquo;s largest job search platform, \u0026ldquo;Indeed\u0026rdquo; as the data source. Only data science-related and full-time positions in the London area are crawled to avoid burdening the server. Only recent data is collected, new posts added daily are automatically crawled in the daily schedule, and have the flexibility to add other platforms or other types of vacancies in the future.\nWhy distributed?  Increase the efficiency of the crawler. When the number of crawlers increases, a single computer may be unable to handle it, and if the machine suddenly fails, the crawler will be interrupted. Avoid IP blocking due to excessive crawling (used with sleep in the program).  Celery/RabbitMQ/Flower Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages while providing operations with the tools required to maintain such a system.\nIt’s a task queue focusing on real-time processing and supporting task scheduling.\nThere are three roles in a distributed crawling architecture:\n Producer, which dispatches tasks to the Broker Message Broker, which receives tasks and forwards them to Workers (in this post, I use RabbitMQ as the distributed task forwarding/message delivery center) Worker, which receives tasks from the Broker and executes them (e.g., Crawler)  Process: the producer sends a task to the queue. After the workers (the units that execute tasks provided by Celery) connect to the broker, they can receive tasks from the queue and process them based on the settings.\nThe following figure shows the RabbitMQ overview. We can see that two tasks are waiting for workers in the queue. The page \u0026lsquo;queue\u0026rsquo; also provides information about each queue\u0026rsquo;s status and the tasks\u0026rsquo; details.\nWe can monitor the status of the workers through Flower to see if it is on and the status of the task execution. Currently, only two workers are enabled for this project, which can be increased or decreased according to the needs.\nThe list of tasks executed by the workers. Scheduling Because we need to automatically crawl the new job vacancies of the day, the python package APScheduler is used here to execute tasks. The following code snippet is to set a timer and execute the function \u0026lsquo;sent_crawler_task_DE\u0026rsquo; every day at 12:00 Taipei time from Monday to Sunday\nscheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Taipei\u0026#34;) scheduler.add_job( id=\u0026#34;sent_crawler_task_DE\u0026#34;, func=sent_crawler_task_DE, trigger=\u0026#34;cron\u0026#34;, hour=\u0026#34;12\u0026#34;, minute=\u0026#34;0\u0026#34;, day_of_week=\u0026#34;*\u0026#34;, ) API \u0026amp; Load test FastAPI can be used to create RESTful APIs and automatically generate the OpenAPI file (OAS3), allowing us to test the APIs on the web.\nAfter inputting the position and location parameters, the API will return job vacancy data in JSON format.\nNext, we can try performing load testing on the created API. The ApacheBench (ab) python package can be used for this purpose. It simulates multiple users sending multiple requests to the server simultaneously.\nThe following command can be used to perform load testing on the API service deployed on the local machine (concurrency=10, requests=1000):\napt-get install apache2-utils -y\rab -c 10 -n 1000 'http://127.0.0.1:8888/'\rHere is an example of the output:\nEquest per second: the number of requests that the API can handle per second.\nTime per request: the average time spent on each request (in milliseconds).\nFailed requests: the number of failed requests. This may be related to the stability of the server.\nDashboard In order to have a clear understanding of job vacancies, a dashboard is produced using Redash, an open-source BI system. The advantage is that it is convenient to retrieve data from the DB through SQL query, and then visualize the results (currently supporting Chart, tunnel, map, pivot table, word cloud, etc.). The results are as follows.\nDeployment Since this application requires launching many services (API, scraper, rabbitmq, mysql, redash, etc.), a tool is needed to manage them uniformly. In this case, Docker Swarm is used to manage multiple services as a single virtual system, simplifying the process of deploying and managing complex applications. Docker Swarm allows you to easily scale your applications up or down by adding or removing Docker engines from the cluster, and it provides built-in load balancing and enhanced security features. (We can also use Kubernetes (K8s) to manage multiple services as a single system.)\nThrough the concept of Manager and Worker, multiple machines can be managed, and the Manager can be used to manage all services centrally, including updating services, deploying services, viewing logs, etc., making it very convenient; it is also paired with the portainer UI for management and monitoring.\nAs we can see in the picture below, there are two machines (nodes) in the cluster\nManage all of our services from a single interface.\nDDNS \u0026amp; SSL In order to make your API available to the public, you will need to provide a URL rather than just an IP address. Typically, domain names are purchased from a domain name registrar and come at a cost. You can check the availability and pricing of a domain name by going to a domain name registrar such as domcomp and searching for the domain you are interested in.\nAlternatively, you can use a free dynamic DNS (DDNS) service such as No-IP to provide a URL for your API. With a DDNS service, you can create a domain name that automatically points to your API\u0026rsquo;s IP address, even if it changes. However, you will need to manually renew the domain name each month to keep it active.\nSSL (Secure Sockets Layer) is a security certificate that enables a secure connection between a web browser or computer and a server. After successful installation, an HTTPS symbol will appear next to the URL in the browser. Many organizations issue SSL certificates, and this project uses a free SSL certificate from Let\u0026rsquo;s Encrypt. The certificate is managed using the tool Traefik, which performs reverse proxy and includes features for managing URLs, DNS, load balancing, and automatically renewing SSL certificates. It is also easy to integrate with Docker.\nSource:Traefik\nThe dashboard for Traefik v2.\nCI/CD During the development and iteration of the project, a CI/CD (Continuous Integration/Continuous Deployment) tool can be used to automate the testing and deployment process, reducing the chance of errors. This project uses Gitlab CI/CD, and the Specific runners execute CI/CD commands. Currently, the configuration is set to trigger CI only when a merge request is made. The CI process automatically performs testing, builds a docker image, and uploads it to docker hub. After a new tag is created, the CD process is triggered, and the service is deployed to the docker swarm. Similar CICD tools include CircleCI, GitHub Actions, etc. The advantage of Gitlab is that it is open source and can be deployed on your own server (on-premises) for free.\nFor example, in the API repository, the process would be test -\u0026gt; build -\u0026gt; deploy.\nTest coverage refers to the percentage of your codebase that is executed during testing in a CI process. Monitoring After deployment, it is important to monitor the service to ensure that the operation status of each service and the performance of the machines are as expected. Prometheus can be used to collect time-series data through integration with traefik and node-exporter, and the results can be visualized in a Grafana dashboard.\nSource:Traefik2.2\nIn this case, there are two machines being used, so the node count is 2. Source:Node Exporter for Prometheus Dashboard EN\nResult  Data API  https://bit.ly/3gGBTUX\n Dashboard  The following is an analysis of the number of jobs posted daily for each job type, the average salary of each job type, and the word cloud of the JD in the job postings.\nNote: The data up to May 2022 is used as an example case.\nAdditional Resources  Python 大數據專案 X 工程 X 產品 資料工程師的升級攻略 Celery - Distributed Task Queue  ","permalink":"https://bruceewue.github.io/blog/job_scrape/","tags":["Dashboard","Project","Redash"],"title":"Project - Job Scraping and Analysis"},{"categories":["Machine Learning"],"contents":"After learning machine learning and data analysis methods, I can\u0026rsquo;t help but think: what\u0026rsquo;s next? How can we make the models or analysis results we build usable? This post uses data from the manufacturing industry in the Tianchi Big Data Competition as an example to create a machine learning system, including model building, API service deployment, and system monitoring and utilises a CICD tool to automate the process.\nData source This dataset is about chemical continuous process data, and the goal is to predict Yield, which is a regression problem. In this project, I retrieve data from AWS S3.\nDigital Manufacturing Algorithm Competition of JinNan Tianjin\nModel Deployment When building a machine learning system, the ML code may only take up a small portion; most of the work would be in handling data collection, feature engineering, deployment, system monitoring, etc.\nSource:Hidden Technical Debt in Machine Learning Systems(2015)\nMachine learning modelling follows the data validation process -\u0026gt; Feature engineering -\u0026gt; Model training -\u0026gt; Model deployment. Machine learning systems have more Data + Models than traditional IT systems, making it more challenging to regulate system behaviour. One important thing to note is \u0026ldquo;Reproducibility\u0026rdquo;, which means that the process mentioned above modules must ensure that the same Output is obtained when given the same Input. It is also an important reason we need testing for the system, with the expectation of reducing the risk of the system behaving unexpectedly(Uncertainty).\nTo ensure \u0026ldquo;Reproducibility\u0026rdquo;, there are three ways:\n Use version control to keep track of changes to the code and data. Set seed as much as possible if randomness is required. Use containerisation, such as Docker, to package the application and its dependencies together in a portable container, ensuring that the application runs consistently across different environments.  End-to-end ML system The following diagram shows the process of building an end-to-end ML system, which may iterate continuously. Source: Continuous Delivery for Machine Learning end-to-end process\nThis project references the following architecture to construct a machine learning system.\nBelow are explanations for several points:\nProductionize Model We need to deploy not just the model itself but the entire pipeline from data processing to model deployment. In this part, We will package a series of steps such as feature engineering and modelling through sklearn.pipeline, and give the trained model (.pkl) a version number and package the preprocessing, model training, and configuration into a package and push it to the Python package index server. Commonly used is the PyPI, while here, I use gemfury to create a non-public Package Repository.\nDeployment(Serving the model via API) In the deployment part, this uses the Python 3.6+ supported high-performance asynchronous framework FastAPI, which has the advantage of automatically generating openAPI spec and can easily use Pydantic for schema validation.\nSource: tiangolo/fastapi\nThe following is an example of the API docs In the deployment, the service will be containerised through Docker. Since multiple services (MLAPI, Prometheus, Grafana, cAdvisor) are required to launch, this will start them up at once using docker-compose.\nWhy docker for ML?  Reproducibility: ensures consistent results every time the model is run Scalability: supported by all major cloud platforms and easy to integrate Isolation: ensures independent operation of services and resource isolation (Process Isolation) Maintainability: easy to set up environment and portable, making it easy to share among teams.  Source: SUPERCHARGE YOUR DEVELOPMENT ENVIRONMENT (USING DOCKER)\nDeploy to Cloud The application can be deployed on the cloud to provide services; we can choose PaaS or IaaS. Here, Heroku and AWS are selected to deploy. The difference between PaaS and IaaS is as follows, if you choose IaaS, you will have more parts to manage yourself, but also relatively more flexible.\nSource: RedHat\nElastic Container Registry (ECR): upload the docker images to this storage for other AWS services to access. Amazon Elastic Container Service (ECS): manage Docker containers. The following diagram shows the architecture of ECS. A cluster must be created first, and then the internal service and task should be defined.\nSource: AWS\nThe following diagram shows the cluster created in this case, and a service named \u0026ldquo;custom-service\u0026rdquo; was created.\nMonitoring The dashboard is built by using Prometheus and Grafana. The following diagram shows the Prometheus architecture, which periodically retrieves and stores time series data through the HTTP pull method. As long as the corresponding HTTP interface and the format defined by Prometheus are provided, it can be monitored; it also provides PromQL language for querying and data visualisation through Grafana, and alerts can be set when an anomaly occurs.\nSource: Prometheus\nThis project includes three parts in monitoring.\n Docker Monitoring: used to monitor Docker system resources, using the tool:google/cAdvisor  Source: 8bitmen.com\n The following diagram demonstrates the case, mainly monitoring the CPU and memory of four services.   Model Monitoring: monitoring the predicted values of the model   Number of predicted values per unit of time, whether there is an abnormal Monitor whether the predicted value of the model has significant changes at different times (with Z-score/SEM/STD or other statistical methods)  Log Monitoring: achieved through ELK Stack.   If there is text-type data, Kibana is more suitable than Grafana; time series DB may cause performance problems due to high cardinality.  Data drift problem in the input data is also another one that must be monitored.\nContinuous Integration/Continuous Deployment(CI/CD) The main goal is to automate every stage of application development, testing, and deployment, making the system \u0026ldquo;always releasable\u0026rdquo;, standardised and making the process transparent and easy to trace. Many CI/CD tools are available, and I use CircleCI in this project.\nSource:CircleCI\nFirst, we need a configuration file, \u0026ldquo;config.yml\u0026rdquo;, to define jobs and steps describing each step in the process, including the required environment or settings, commands to be executed, etc. CircleCI can easily connect with Github projects. When a new pull request is created, or subsequent commits are made to the branch, it triggers jobs.\nThe figure below is an example where the application is deployed when the test_app (unit and integration tests for mlapi) pass. Only when it is merged into the main branch and given the commit tag does the model package upload and release.\nThe figure below is an example of the \u0026ldquo;test_app\u0026rdquo; job, where Tox is a Python automated testing tool that can be integrated with Pytest.\nAdditional Resources  Deployment of Machine Learning Models - Online Course Testing and Monitoring Machine Learning Model Deployments - Online Course  ","permalink":"https://bruceewue.github.io/blog/ml_deployment/","tags":["MLOps","AWS","Project","Prometheus","Grafana"],"title":"Project - ML Model Deployment"},{"categories":["Data Engineering"],"contents":"自己來動手製作的COVID-19 Dashboard吧，本篇將透過GCP雲端服務自動抓取最新的疫情資料，並使用Google的可視化工具Data studio串接來製作每日自動更新的疫情儀表板。\nData source CSSEGISandData/COVID-19\nDiagram 透過Cloud Scheduler定時觸發Cloud Functions中部署的爬蟲程式,並將資料儲存至Cloud Storage，並由Data Studio即時讀取最新資料並製作COVID-19即時資訊儀表板。\nGCP Source: Google\n這次採用的雲端平台GCP內含相當多的服務可以使用，目前有提供300美金3個月的免費額度可使用；而本篇使用到的服務主要有Cloud Storage, Cloud Functions, Cloud Scheduler\nCloud Functions 利用Cloud Functions來新增一個可透過Http request觸發的爬蟲程式，記得在requirements.txt 寫入需安裝的套件(此處為pandas以及google-cloud-storage)，\n爬取確診人數的部分如下，也可以新增迴圈連同死亡、復原的人數一起抓取。\nimport pandas as pd def crawler(request): # get data from github df = pd.read_csv(\u0026#34;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026#34;) # wide format to long format df = pd.melt(df, id_vars=df.columns[:4], value_vars=df.columns[4:], var_name=\u0026#34;date\u0026#34;, value_name=\u0026#34;count\u0026#34;) return df.to_csv(index=False) 由於需要抓取自動後上傳至Cloud Storage，所以還需加入以下網址內程式碼 Uploading objects\nNote:\n Cloud Functions只允許在 /tmp路徑下才能存檔。 Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Storage 雲端儲存空間，新增bucket後即可開始使用，也能像google drive一樣手動上傳檔案，但Cloud Storage的優勢是更能方便地在GCP上與各服務串接；免費的部分有5GB的額度。\n觸發Cloud functions後可以確認看看是否有確實上傳確診、死亡、復原的資料 Note:\n Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Scheduler 可以依據原始資料的更新時間來設定希望多久執行一次爬蟲，設定方式是像在Linux中的Crontab排程，使用Cron語法，可以利用這個網站確認自己撰寫的排程是否符合預期。\n設定完成的狀態如下 Note:\n Cloud Scheduler完全免費方案只有設定3個Job的額度  Data Studio 在設定好資料後就可以嘗試來把它視覺化了，這裡採用的是google免費的Data Studio，算是滿容易上手的工具，跟GCP串接當然也是沒有問題的。\n此處新增資料源-\u0026gt;從Data Storage內選取三份csv檔案\nResult COVID-19 Dashboard Animated Bar Chart  即時資訊互動儀表板如下，可自由選擇Country/Region (建議可點擊右下角\u0026quot;Google數據分析\u0026quot;觀看完整報表)  \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rAdditional Resources  打造動態報表！雲端 Python 爬蟲資料流  ","permalink":"https://bruceewue.github.io/blog/covid_dashboard/","tags":["Data studio","GCP"],"title":"Create COVID-19 Dashboard by Data studio"},{"categories":["Others"],"contents":"Markdown Example\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n  Steve Francia test\n  List  有序1  四個空格內縮   有序2 有序3  Unordered List  無序1  四個空格內縮   無序2  Blockquote  This is a blockquote example.\n Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) No language indicated, so no syntax highlighting. But let's throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;.\rInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nTables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    Youtube embeded   ","permalink":"https://bruceewue.github.io/blog/markdown_test/","tags":["Hugo"],"title":"Markdown test"}]