[{"categories":["Projects"],"contents":"Table of Contents  The Essence of TFL\u0026rsquo;s Cycling Data The Architectural Blueprint The Dashboard: A Window to Cycling Trends Setting the Stage: Pipeline Implementation  To optimize the provided blog content on \u0026ldquo;London\u0026rsquo;s TFL cycling data pipeline\u0026rdquo;, I suggest several enhancements for SEO and reader engagement. Here\u0026rsquo;s the revised blog in Markdown format:\n London\u0026rsquo;s TFL Cycling Data Pipeline: A Comprehensive Data Engineering Project Understanding urban cycling trends is crucial for developing efficient and sustainable city transportation systems. In this blog, we delve into a cutting-edge data engineering project that utilizes London\u0026rsquo;s Transport for London (TfL) cycling data to unravel the intricacies of cycling patterns in this bustling metropolis.\nData Set: A Closer Look The data, sourced from TfL\u0026rsquo;s Cycling Data, comprises comprehensive CSV-formatted records. Here\u0026rsquo;s a snapshot of the data structure:\n   Column Name Description     Rental Id A distinct identifier for each ride   Duration The length of time the ride lasts   Bike Id A distinct identifier for the bicycle   End Date The time at which the ride ended   EndStation Id A distinct identifier for the destination station   EndStation Name The name of the station where the ride ends   Start Date The time at which the ride started   StartStation Id A distinct identifier for the origin station   StartStation Name The name of the station where the ride begins    Pipeline Overview: Integrating Advanced Tools and Technologies Our project harnesses a plethora of sophisticated tools and technologies to forge a robust end-to-end data engineering pipeline:\n Google Cloud Platform (GCP): The backbone for data storage, processing, and analytics. Google Cloud Storage (GCS): Our data lake for raw and processed datasets. Google Cloud Dataproc: A powerhouse for fast data processing via Apache Spark and Hadoop. BigQuery: A serverless, scalable data warehouse tailor-made for analytics. Terraform: Orchestrating GCP resources with Infrastructure as Code (IaC). Apache Airflow: The conductor for our data pipelines. PySpark: Python\u0026rsquo;s gift to big data processing with Apache Spark. Looker Studio: Where data transforms into interactive visual stories.  The Pipeline in Action: Steps for Streamlined Data Processing a. Data Ingestion:\n Implementation of Apache Airflow for scheduling and automating data ingestion. Conversion of CSV files to Parquet format for efficiency, stored in GCS.  b. Data Transformation:\n Utilization of PySpark and Google Cloud Dataproc for cleaning, filtering, and aggregating data.  c. Data Storage:\n Leveraging Google BigQuery for storing transformed data, enabling real-time analytics.  d. Data Visualization:\n Employing Looker Studio for crafting interactive dashboards, providing profound insights into London\u0026rsquo;s cycling trends.  Interactive Dashboard: Visualizing Insights A comprehensive dashboard, designed in Looker Studio, brings data-driven narratives to life, aiding stakeholders in informed decision-making.\nDashboard Link\nSetting Up the Pipeline: Your Step-by-Step Guide   Repository Cloning:\nStart by cloning the GitHub repository:\ngit clone https://github.com/bruceewue/de_zoomcamp_tfl_cycling.git\n  Google Cloud Platform Setup:\nCreate a new project and authenticate your account with the Google Cloud SDK.\n  Terraform Initialization:\nInstall Terraform and initialize the configuration to set up GCP resources.\nterraform init terraform plan terraform apply   Apache Airflow Configuration:\nCustomize your docker-compose.yml with GCP Project ID and GCS bucket information.\nAirflow Setup Reference\nNext, open your web browser and navigate to the Airflow web server URL: http://localhost:8080, and manually trigger a task run, click the play button located next to the DAG name Once your DAG runs successfully, you will see that the Parquet files have been stored in your data lake, which is located in Google Cloud Storage (GCS).\n  PySpark \u0026amp; Dataproc Activation:\nEnable the Dataproc API, create a cluster, and execute PySpark jobs for data transformation.\ncd spark # Upload PySpark file to GCS gsutil cp spark_sql_big_query.py gs://GCP_GCS_BUCKET/code/spark_sql_big_query.py # Submit a PySpark job gcloud dataproc jobs submit pyspark\\  --cluster=YOUR_CLUSTER \\  --region=europe-west1 --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar gs://cycling_data_lake_cycling-uk/code/spark_sql_big_query.py \\  -- \\  --input=gs://GCP_GCS_BUCKET/raw/\\  --output=YOUR_BIGQUERY_TABLE   Once the PySpark job completes successfully, the transformed data will be stored in the specified BigQuery table.\n","permalink":"https://bruceewue.github.io/blog/project-london-cycling-analytics/","tags":["Dashboard","Project","GCP"],"title":"Project - London Cycling Analytics"},{"categories":["Projects"],"contents":"In today\u0026rsquo;s fast-paced digital landscape, integrating various cloud services into small projects has become crucial for streamlined and efficient project management. This article delves into the world of Amazon Web Services (AWS), providing a comprehensive guide on how to harness its capabilities for your projects. We will explore how to set up and manage key AWS services, including IAM for user and policy configuration, VPC for network setup, RDS for database management, EC2 for server instances, and S3 for storage solutions.\nTable of Contents  Table of Contents 1. Setting Up IAM Users Policies, and User Group: 2. Configuring VPC and Subnets: 3. Security Group and RDS Configuration: 4. EC2 Instance Configuration: 5. S3 Storage Configuration and Usage: 6. Frontend and backend Integration: 7. Pausing and Resuming Projects: 8. Resource Cleanup: 9. Data Backup:  1. Setting Up IAM Users Policies, and User Group: In the world of AWS, security is paramount. This step involves creating new IAM policies, granting specific access permissions to services like EC2, VPC, RDS, and S3. It\u0026rsquo;s essential to configure roles and permissions related to EC2 and RDS, focusing on capabilities like ListRole, CreateRole, PassRole, and ListInstanceProfiles. To effectively demonstrate a demo, we\u0026rsquo;ll create two IAM users: \u0026lsquo;bruce-demo\u0026rsquo; and \u0026lsquo;kuang-demo\u0026rsquo;. These users will be assigned appropriate policy permissions, serving as practical examples in our demonstration.\nPolicies: User Group: 2. Configuring VPC and Subnets: The creation of a VPC, dubbed \u0026lsquo;aws-demo-vpc\u0026rsquo; with an IP range of 10.0.0.0/16, lays the foundation of your network infrastructure in AWS. Subnet configuration, including the setup of public and private subnets, plays a significant role. Establishing an Internet Gateway (IGW) for the public subnet and configuring the Route Table ensures seamless internet connectivity.\nVPC: Subnets: IGW: 3. Security Group and RDS Configuration: Security is not just a feature but a necessity. Setting up a security group named \u0026lsquo;aws-demo-review-rds-sg\u0026rsquo; with appropriate inbound and outbound rules facilitates the connection to a MySQL database. The creation of a Subnet Group and a database instance named \u0026lsquo;aws-demo-rds\u0026rsquo; in RDS, with MySQL as the database engine, enhances the security posture. Adjusting settings like IOPS, storage auto-scaling, and Multi-AZ configurations ensures a robust and resilient database setup.\nSecurity Group: RDS_SG: RDS: 4. EC2 Instance Configuration: Launching EC2 instances is a crucial aspect of this setup. We initiate two instances, both derived from the Amazon Linux AMI. These are placed within the public subnet and assigned public IP addresses to ensure network accessibility. To tailor their accessibility, we configure security groups, setting up distinct ports for each: port 8080 for the first instance (EC2-1), designated for front-end simulation, and port 3000 for the second instance (EC2-2), intended for back-end simulation.\nThe functionality of these instances is further enhanced by installing Docker and docker-compose, which aid in the deployment and management of applications, along with GIT for version control. This arrangement effectively creates a practical environment, where one VM (EC2-1) acts as a front-end server and the other (EC2-2) as a back-end server, demonstrating a typical web application infrastructure.\n5. S3 Storage Configuration and Usage: The configuration of S3 storage is vital for managing resources effectively. In EC2-1, creating an S3 Bucket for storing frontend resources, such as cover images, and implementing version control to prevent data loss demonstrates a well-thought-out storage strategy. Configuring IAM roles for EC2 instances to interact with S3 services further integrates your storage solutions.\nS3: 6. Frontend and backend Integration: The photograph provided illustrates a successful implementation using AWS services, particularly with EC2 instances and RDS.\nIn this setup, EC2-2 (backend) plays a crucial role in connecting to the RDS (Relational Database Service) to retrieve data. This data management is fundamental, as RDS offers scalable and efficient database storage, essential for dynamic web applications.\nMeanwhile, EC2-1 (frontend) is configured to fetch database data through the backend API. This architecture allows the front-end of the application to display data on the webpage dynamically and interactively. The separation of concerns between the frontend and backend ensures a more secure, efficient, and manageable system.\nAdditionally, the use of Amazon S3 (Simple Storage Service) is highlighted for image hosting. By storing images in S3 and retrieving their public OBJECT URLs, the application can easily embed these images on the web pages. This approach not only simplifies the management of web resources but also leverages the high availability and durability of AWS S3.\nFurthermore, the interface includes a dialog box where users can insert new data into the database directly from the webpage. This feature enhances user interaction and data input efficiency.\nEC2-1 (frontend): EC2-2 (backend): 7. Pausing and Resuming Projects: The ability to pause and resume resources efficiently is crucial. Stopping RDS and EC2 resources, creating snapshots and AMIs, and restoring databases and instances from these snapshots and AMIs are essential for maintaining service continuity. Updating connection parameters and restarting services ensures minimal downtime and efficient resource management.\nRDS-Snapshot: 8. Resource Cleanup: Finally, effective resource management involves cleaning up after project completion. Deleting RDS instances, snapshots, and subnet groups; terminating EC2 instances and deleting related AMIs and snapshots; removing S3 buckets; and cleaning up the VPC and associated resources are critical steps to avoid unnecessary costs and maintain a clutter-free AWS environment.\n9. Data Backup: Regularly creating snapshots or AMIs of RDS and EC2 ensures that you are prepared for disaster recovery. This proactive approach to data backup is key to maintaining data integrity and business continuity in the face of unforeseen challenges.\nThis guide offers a structured approach to setting up and managing your AWS environment, ensuring that each component is configured for optimal performance and security. By following these steps, you can establish a robust and scalable AWS infrastructure tailored to your specific project needs.\n","permalink":"https://bruceewue.github.io/blog/project-building-a-small-project-with-aws/","tags":["AWS","Project"],"title":"Project - Building a small project with AWS"},{"categories":["Data Engineering"],"contents":"When it comes to working with data, many challenges arise. One of the biggest challenges is building a reliable and scalable data pipeline that can handle large volumes of data while ensuring the data\u0026rsquo;s accuracy and consistency. With dbt, you can transform, test, and document your data in a reproducible and scalable way. In this article, I\u0026rsquo;ll introduce dbt and show you how to utilise it with BigQuery.\nProject Diagram Source: DE_zoomcamp\nChallenges In the past, data engineering and analytics were distinct fields with unique tools, languages, and skill sets. Data pipelines that are operated on platforms like Apache Spark or Amazon Redshift are orchestrated by data engineers using tools like Apache Airflow or AWS Glue. To query and alter data that is stored in databases or data warehouses, data analysts utilise technologies like Python or SQL.\nHowever, this separation has some drawbacks:\n It separates data teams into silos, which makes it difficult for them to collaborate and align. Managing numerous tools and platforms adds complexity and overhead. It reduces agility and speed in delivering data products and insights.  To address these challenges, a new paradigm has emerged: Analytics Engineering.\nAnalytics engineering applies software engineering principles and techniques to data transformation and analysis. Analytics engineering aims to bridge the gap between data engineering and analytics by enabling data teams to work directly within the data warehouse using a single tool: dbt.\nWhat is dbt (data build tool)? Source: dbt\ndbt is an open-source command-line tool that enables data analysts and engineers to transform raw data into valuable insights. It assists users in managing their entire data pipeline, from extracting data from sources to transforming and loading it into a target data warehouse. With dbt, users can build, test, and deploy data pipelines as code, making it easier to maintain and scale their data infrastructure.\ndbt is designed to work with SQL-based data warehouses, such as BigQuery, Redshift, Snowflake, and others. It uses a declarative syntax to define data transformations, making it easy for non-technical stakeholders to understand what the data is doing. dbt also has a testing framework that enables users to ensure data quality and consistency throughout the pipeline.\nSource: dbt\nWhat are the features of dbt? Modularity: dbt allows you to break down your data pipeline into smaller, reusable components, which can make it easier to maintain and scale over time.\nVersioning: dbt uses Git to manage versions of your code, which makes it easy to track changes and collaborate with others.\nTesting: dbt provides a testing framework that allows you to test your data pipeline at different stages of the pipeline, ensuring the accuracy and consistency of your data.\nDocumentation: Documentation is generated automatically from our models, sources,and tests using Markdown syntax. Documentation provides metadata such as descriptions, tags, columns types, etc.,as well as lineage diagrams that show how datasets depend on each other.\nSeeds: Seeds are CSV files that contain static or reference data such as lookup tables or configuration parameters. Seeds allow us to load small amounts of data into the warehouse without using external sources.\nPackages: Packages are collections of models,sources,tests,macros etc.,that can be shared across projects or with the community via GitHub or other platforms. Packages allow us to leverage existing code or best practices from other users or organizations.\nUsing dbt Cloud dbt Cloud is a web-based application that helps data teams configure, deploy, and control dbt project. dbt Cloud leverages the power of dbt Core and adds features such as:\nIntegrated development environment (IDE): A web-based interface that allows you to write, test, and run dbt models with syntax highlighting, autocomplete, error checking, and version control integration.\nScheduling: A feature that lets you schedule dbt jobs to run automatically at specified intervals or triggers. You can also monitor the status of your jobs and get alerts for failures or anomalies.\nDocumentation: A feature that generates interactive documentation for your dbt project based on your code comments and metadata. You can explore your data models, view lineage graphs, query results, tests, sources.\nMonitoring \u0026amp; alerting: A feature that helps you track the performance and quality of your data models with metrics such as run duration, freshness, row count, test results etc.\nGet started with dbt cloud First, we must create a dbt cloud account and set up the project by connecting to my data warehouse(BigQuery) and Github. Then, click \u0026ldquo;initialize your project\u0026rdquo; on dbt Cloud Here, we\u0026rsquo;ll be able to configure your project settings(dbt_project.yml).\nOnce you\u0026rsquo;ve set up your project settings, you can build and deploy your data pipeline using dbt Cloud. We\u0026rsquo;ll need to create a series of data models in your dbt project to do this.\nTo create a new data model, simply create a new SQL file in your dbt project directory and define your data model using SQL. You can also define your data model using YAML files, which define the dependencies between your models.\nOnce you\u0026rsquo;ve defined your data models, you can use command dbt run to compile your data models into SQL queries and applies any transformations to your data. It checks for any changes in your source data and creates the final output. Essentially, it runs your data pipeline end-to-end.\ndbt lineage graph is a visual representation of the dependencies and relationships between data sources, models, and outputs in a dbt project1. It is based on the concept of a directed acyclic graph (DAG), which is a way of modeling data transformations as a series of nodes and edges1. A dbt lineage graph can help you understand how your data flows from raw sources to final reports, identify potential issues or inefficiencies in your project structure, and document your data pipeline for other stakeholders.\nDeployment To deploy your dbt project using dbt cloud, we need to follow these steps\n Create a deployment environment by selecting “Deploy” and then “Environments”. Name your environment, add a target dataset, and select an account type. Schedule a job by selecting “Deploy” and then “Jobs”. Name your job, select an environment, choose a run frequency, and add any custom commands or notifications. Deploy your project by clicking on the “Deploy” button in the dbt cloud UI. You can also trigger a manual run by clicking on the “Run” button.  Here we can see the document:\nData studio Finally, we can use Google Data Studio to connect BigQuery and present the transformed data through the dashboard.\nConclusion dbt is a powerful tool that helps data teams manage complex data pipelines more efficiently. By offering a user-friendly online interface, automated deployment tools, and enterprise-level security features, dbt Cloud expands the capability of dbt. With dbt and dbt Cloud, data teams can collaborate more effectively, deploy changes faster, and ensure data quality with confidence. If you are looking for a fast and reliable way to transform your data warehouse using SQL, then you should give dbt Cloud a try. It will help you streamline your workflow, improve collaboration,and deliver trusted insights faster.\nAdditional Resources  DE-zoomcamp_week4_analytics_engineering  ","permalink":"https://bruceewue.github.io/blog/de_dbt/","tags":["GCP","BigQuery","dbt","Data Studio","dezoomcamp"],"title":"Analytics Engineering (dbt + BigQuery + Data Studio)"},{"categories":["Machine Learning"],"contents":"In this article, I\u0026rsquo;ll explore how to deploy an existing Keras model to AWS-EKS using TensorFlow Serving and Kubernetes. TensorFlow Serving is a flexible and high-performance system for serving machine learning models, and Kubernetes is an open-source container orchestration system that automates the deployment, scaling, and management of containerized applications.\nTo deploy a Keras model to AWS-EKS, we will first need to package the model using TensorFlow Serving. We will then create a Kubernetes deployment that includes a container running the TensorFlow Serving image, which will serve our model. The diagram below illustrates the architecture of the deployment, with the yellow circles indicating the components that will be deployed to Kubernetes.\nIn this project, we have two key components:\n  Gateway, which downloads images, resizes them, and turns them into a numpy array. This is computationally inexpensive and can be done using a CPU.\n  Model, which involves matrix multiplications that are computationally expensive. Therefore, we might need to use GPUs to handle this component.\n  The advantage of this setup is that the two components can be scaled independently. For example, we may need five gateway instances but only two instances of TF Serving with high-performance GPUs to handle the computationally expensive model component.\nTensorFlow Serving TensorFlow Serving is an open-source platform for serving machine learning models. It provides a flexible, high-performance serving system for deploying TensorFlow models in production environments. With TensorFlow Serving, you can quickly deploy new models and run experiments while maintaining the same server architecture and APIs. It provides a gRPC interface for clients to send prediction requests and receive responses.\nFirst, convert the existing Keras model.\n# convert.py import tensorflow as tf from tensorflow import keras model = keras.models.load_model(\u0026#39;./clothing-model.h5\u0026#39;) # Convert the model to TF format tf.saved_model.save(model,\u0026#39;clothing-model\u0026#39;) Containerization Next, we need to containerize the model and gateway, and create separate Docker images for each. To enable communication between the gateway and model containers, we can use docker-compose.\n#image-model.dockerfileFROMtensorflow/serving:2.7.0COPY clothing-model /models/clothing-model/1ENV MODEL_NAME=\u0026#34;clothing-model\u0026#34;#image-gateway.dockerfileFROMpython:3.8.12-slimRUN pip install pipenvWORKDIR/appCOPY [\u0026#34;Pipfile\u0026#34;, \u0026#34;Pipfile.lock\u0026#34;, \u0026#34;./\u0026#34;]RUN pipenv install --system --deployCOPY [\u0026#34;gateway.py\u0026#34;, \u0026#34;proto.py\u0026#34;, \u0026#34;./\u0026#34;]EXPOSE9696ENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;--bind=0.0.0.0:9696\u0026#34;, \u0026#34;gateway:app\u0026#34;]Kubernetes (K8s) Source: kubernetes.io\nKubernetes is an open-source platform for automating the deployment, scaling, and management of containerized applications. It provides a unified API for deploying and managing containers, and it can run on a variety of cloud platforms and on-premise servers. With Kubernetes, you can automate the deployment and scaling of your applications, and ensure that your services are always available and up-to-date.\nHere are the key components and features of Kubernetes:\n  Nodes: Nodes are the worker machines in a Kubernetes cluster, where the applications and services are deployed. Nodes can be physical machines or virtual machines running in a cloud environment.\n  Pods: Pods are the smallest and simplest unit in the Kubernetes object model. They contain one or more containers, and all containers in a pod share the same network namespace, making it possible for them to communicate with each other using localhost. The pod runs on a node.\n  Services: The entry point of an application and route requests to pods. Services are abstractions that define a set of pods and provide a stable IP address and DNS name to access them. Services also provide load balancing and support for rolling updates, making it easy to deploy new versions of an application.\n  Deployments: A Deployment is a higher-level object in Kubernetes that manages a set of replicas of a pod. The Deployment provides a declarative way to manage the desired state of a set of pods, and ensures that the desired number of replicas are always running and available.\n  Horizontal Pod Autoscaling (HPA) is a Kubernetes feature that allows you to automatically scale the number of replicas of a pod based on CPU utilization, memory utilization, or custom metrics. This helps to ensure that your application can handle varying levels of traffic and demand without manual intervention.\n  In Kubernetes, an Ingress is an API object which is a powerful and flexible way to manage external HTTP(S) traffic to your internal services in the cluster, allowing you to easily expose and route traffic to your services while maintaining control over security and routing policies. (External = Load Balancer, Internal = Cluster IP)\n  Before we start, we need to install Kubectl and Kind.\n  kubectl: kubectl is the command-line tool for interacting with a Kubernetes cluster. With kubectl, you can perform a variety of tasks, such as creating and managing resources, accessing logs and status information, and debugging applications. kubectl is a versatile and powerful tool that is an essential part of working with Kubernetes.\n  kind: kind is a tool for running Kubernetes clusters on local development machines. kind makes it easy to spin up a fully-functional Kubernetes cluster on your own machine, which is useful for testing and development purposes. With kind, you can experiment with different configurations, test new features, and debug issues without having to rely on a remote cluster.\u0026rsquo;\n  Deploying TensorFlow Models to Kubernetes -Model-\n# load a Docker image into a Kind cluster. $ kind load docker-image zoomcamp-10-model:xception-v4-001 # list the pods in a Kubernetes cluster. $ kubectl get pod $ kubectl apply -f kube-config/model-deployment.yaml $ kubectl port-forward tf-serving-clothing-model-548c6-fd5n8 8500:8500 # apply a configuration file to a Kubernetes cluster $ kubectl apply -f kube-config/model-service.yaml # forward traffic from a local machine to a service in a Kubernetes cluster $ kubectl port-forward service/tf-serving-clothing-model 8500:8500 -Gateway-\n$ kind load docker-image zoomcamp-10-gateway:001 $ kubectl apply -f kube-config/gateway-deployment.yaml $ kubectl port-forward gateway-7d467-r8dkc 9696:9696 # need to add \u0026#39;type: LoadBalancer\u0026#39; in gateway-service.yaml $ kubectl apply -f kube-config/gateway-service.yaml $ kubectl port-forward service/gateway 8080:80 Deploying to EKS # eks-config.yamlapiVersion:eksctl.io/v1alpha5kind:ClusterConfigmetadata:name:mlzoomcamp-eksregion:eu-west-1nodeGroups:- name:ng-m5-xlargeinstanceType:m5.xlargedesiredCapacity:1# create an Amazon EKS cluster $ eksctl create cluster -f eks-config.yaml # create an Elastic Container Registry (ECR) repository for doecker images $ aws ecr create-repository --repository-name mlzoomcamp-images 用 After running this line of code, you will see the \u0026ldquo;repositoryUri\u0026rdquo;. Send a request through the following code.\n# testing.py import requests url=\u0026#39;4290xxxxx.ap-northeast-1.elb.amazonaws.com/predict/\u0026#39; data = {\u0026#39;url\u0026#39;,\u0026#39;http://bit.ly/mlbookcamp-pants\u0026#39;} result=requests.post(url,json=data).json() print(result) We can see the newly added cluster in EKS; besides, you can also find the added NODE in EC2, as well as the Load Balancer.\nAdditional Resources  mlbookcamps-kubernetes  ","permalink":"https://bruceewue.github.io/blog/ml_k8s_eks/","tags":["AWS","Kubernetes"],"title":"Model Deployment (K8s and EKS)"},{"categories":["Data Engineering"],"contents":"Table of Contents  Project Diagram Setting Up the Environment Data Acquisition MongoDB: The Database Choice  Python Integration with MongoDB   Kafka: Streamlining Data Flow Airflow: Automating ETL Processes  Additional Resources    Project Diagram Setting Up the Environment Ubuntu 20.04 (WSL2) alongside Python 3.8 for our setup.\nData Acquisition Source: CoinCap\nWe leverage the CoinCap API (coincap_api) to fetch real-time Bitcoin price data.\nMongoDB: The Database Choice MongoDB, a NoSQL database, is preferred for its ability to handle high-velocity and voluminous data efficiently, making it suitable for big data applications. Unlike traditional relational databases, MongoDB supports unstructured data and is schema-free.\nSource: MongoDB\nA comparison between relational databases and MongoDB: Source: RDBMS_MongoDB\nIn MongoDB, data is stored in BSON (Binary JSON) format, and \u0026ldquo;_id\u0026rdquo; is an object, not a string. For seamless data extraction, conversion to a string is advisable.\nStudio3T (https://studio3t.com/), a free MongoDB GUI, can be used for database connectivity.\nPython Integration with MongoDB import pymongo import pandas as pd client = pymongo.MongoClient(\u0026#39;localhost\u0026#39;, 27017) database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Collection Name # Fetch data from MongoDB to Pandas DataFrame query = {} records = pd.DataFrame(list(collection.find(query))) # Insert data into MongoDB df = pd.DataFrame({\u0026#39;price\u0026#39;: [\u0026#39;12.4\u0026#39;, \u0026#39;1.4\u0026#39;, \u0026#39;2.6\u0026#39;]}) records = df.to_dict(\u0026#39;records\u0026#39;) collection.insert_many(records) Kafka: Streamlining Data Flow Apache Kafka, an eminent open-source distributed event streaming platform, is utilised for managing real-time data streams. Its application helps circumvent potential bottlenecks associated with direct data storage in databases.\nSource: Kafka\nThe implementation involves three key steps:\n Publishing and subscribing to event streams, including data import/export. Storing event streams durably. Processing events as they occur or retrospectively.  Start Zookeeper and Kafka services:\nbin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties Create a new Kafka topic:\nbin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1  Producers: Applications that publish events to Kafka. Consumers: Applications that subscribe to and process these events.  # producer.py from kafka import KafkaProducer import requests import json from datetime import datetime brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; producer = KafkaProducer(bootstrap_servers=[brokers]) url = \u0026#34;https://api.coincap.io/v2/assets/bitcoin\u0026#34; while True: response = requests.get(url) value = json.dumps(response.json()).encode() key = datetime.now().strftime(\u0026#34;%Y-%m-%d%H:%M:%S:%f\u0026#34;).encode() producer.send(topic, key=key, value=value) # time.sleep(5) The consumer section receives data and stores it in MongoDB:\n# consumer.py from kafka import KafkaConsumer import pymongo import json brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; consumer = KafkaConsumer(topic, bootstrap_servers=[brokers]) client = pymongo.MongoClient(\u0026#34;localhost\u0026#34;, 27017) database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Collection Name for msg in consumer: dict_data = json.loads(msg.value) dict_data[\u0026#34;data\u0026#34;][\u0026#34;timestamp\u0026#34;] = msg.key.decode() collection.insert_many([dict_data[\u0026#34;data\u0026#34;]]) Airflow: Automating ETL Processes Apache Airflow is a powerful tool for authoring, scheduling, and monitoring workflows. Our use case involves automating the conversion of Bitcoin prices from USD to GBP, followed by database storage.\nSet up the connection to MongoDB via the Airflow Web UI: Define ETL tasks in the DAG:\n Extract: Fetch data from the source. Transform: Convert USD prices to GBP. Load: Store the transformed data in MongoDB.  These tasks are scheduled to run sequentially at 00:30 daily. Use crontab.guru for setting the schedule_interval.\n dags/dag.py   Task Sequence: task1 -\u0026gt; task2 -\u0026gt; task3 Log: Result: Additional Resources  Kafka Documentation Airflow Documentation  ","permalink":"https://bruceewue.github.io/blog/etl/","tags":["Airflow","Kafka","MongoDB"],"title":"Optimising Data Pipelines for Cryptocurrency Analysis: Airflow, Kafka, MongoDB"},{"categories":["Machine Learning"],"contents":"Greater model parameters and network design are always preferred for better performance during model training. Manually tuning these parameters and architecture can be difficult and time-consuming, so in this post, I\u0026rsquo;ll attempt to use an AutoML tool called Autokeras to accelerate this process.\nAutoKeras AutoKeras is an open-source AutoML tool based on Keras that automates designing and tuning machine learning models. It allows users to quickly and easily train high-performance models without extensive expertise in machine learning. By using Autokeras, we can save time and effort compared to manual tuning and train high-quality models for a variety of tasks.\nAutoKeras uses Efficient Neural Architecture Search (ENAS) to automate the process of designing and tuning machine learning models. It also includes support for a variety of preprocessing methods, such as normalization, vectorization, and encoding. AutoKeras can be used for a wide range of tasks, including structured data, image data, time series prediction, and multitasking.\nSource: AutoKeras\nThe official website of AutoKeras provides examples that show how to use the tool in just a few lines of code.\nimport autokeras as ak # Initialize the image classifier. clf = ak.ImageClassifier() # Feed the image classifier with training data. clf.fit(x_train, y_train) # Predict with the trained model. predicted_y = clf.predict(x_test) Environment With Colab Pro, some Python modules and GPU are already installed and configured in Colab so that we can focus on coding.\nExample 1 - Fraud detection The first case is a credit card fraud detection on Kaggle, which is a structured data classification problem\ndataset: Kaggle-Data-Credit-Card-Fraud-Detection (284807 rows × 31 columns)\nThe StructuredDataClassifier is used here to handle the structured data classification problem.\n# StructuredDataClassifier cbs = [ # Switch to the next model after three consecutive training cycles without improvement # monitor default: val_loss tf.keras.callbacks.EarlyStopping(patience=3,monitor=\u0026#39;val_loss\u0026#39;), # histogram_freq=1 means to count once in each training cycle, and display the weight and bias distribution of each layer of neurons during training in a histogram tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1) ] # max_trials is the number of models to try, the default value is 100, you can find the structure of the default model in oracle.json in the directory clf=ak.StructuredDataClassifier(max_trials=3) # The default value of epoch is 1000, and it may end early when using earlystopping history=clf.fit(x_train,y_train,callbacks=cbs,epochs=25) In addition to directly using StructuredDataClassifier, if you need to customize the model architecture, search space, etc., you can also use AutoModel to further design the model, as shown below.\ninput_node = ak.StructuredDataInput() output_node = ak.StructuredDataBlock()(input_node) output_node = ak.ClassificationHead()(output_node) clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3) Save and load models:\n# Save model try: # TensorFlow SavedModel format model.save(\u0026#39;model_autokeras\u0026#39;,save_format=\u0026#39;tf\u0026#39;) except: # keras h5 format model.save(\u0026#39;model_autokeras.h5\u0026#39;) # Load AutoKeras model from tensorflow.keras.models import load_model loaded_model = load_model(\u0026#39;./auto_model/best_model\u0026#39;, custom_objects=ak.CUSTOM_OBJECTS) Training process (Colab)  If the training is interrupted and executed again, it will continue the training based on the records After the training is completed, AutoKeras will use all training set + validation set for training again with the best model. After all trials are completed, the model with the best performance will be automatically stored in best_model/* Due to the problem of category imbalance in this data set, Autokeras also supports setting class_weight in fit() for weighting the loss function, so that the model will pay more attention to minority categories.  Training Visualization - TensorBoard To understand the training process, you can use tensorboard to read the log data stored in callback to visualize the training process; use the tensorboard.dev to generate a public page for sharing with others.\nimport tensorflow as tf import datetime from tensorboard.plugins.hparams import api as hp !tensorboard dev upload --logdir logs \\ --name \u0026#34;Simple experiment\u0026#34; \\ --description \u0026#34;fraud detect\u0026#34; \\ Execute the above program fragment and check \u0026ldquo;Reload data\u0026rdquo; before training, and the data will be updated during training.\nAccuracy and loss changes during training Model architecture ClearML If you want to monitor and track the results and share the experiments with people, you can also use the MLOps platform ClearML that supports AutoKeras to present it on an interactive web page.\nfrom clearML import Task # register at https://app.community.clear.ml # get the \u0026#39;key\u0026#39; and \u0026#39;secret\u0026#39;： Task.set_credentials( api_host=\u0026#39;https://api.community.clear.ml\u0026#39;, web_host=\u0026#39;https://app.community.clear.ml\u0026#39;, files_host=\u0026#39;https://files.community.clear.ml\u0026#39;, key=\u0026#39;xxxxx\u0026#39;, secret=\u0026#39;xxxxxx\u0026#39; ) # Create a new ClearML task # Visit https://app.community.clear.ml/dashboard task = Task.init(project_name=\u0026#39;autokeras age\u0026#39;, task_name=\u0026#39;age regressor\u0026#39;) The project overview is on the ClearML main page In addition to the loss and accuracy during training, it also includes the machine\u0026rsquo;s status. Changes in weight and bias in the model  ClearML will also show the results through the library matplotlib and seaborn  Example 2 - Age prediction This example is about age prediction, which is a regression problem to predict age from photos.\n(Dataset: wiki_crop)\nAfter image filtering and pre-processing, the images were converted to 52984 128*128 images.\nTraining data: Use AutoModel to customize model details, which is similar to Keras functional API. AutoKeras then finds the best result from the conditions we give.\ninput_node = ak.ImageInput() # If just want to try EfficientNet b0~b7 output_node = ak.ImageBlock(block_type=\u0026#39;efficient\u0026#39;, normalize=True, augment=True)(input_node) output_node = ak.RegressionHead(droupout=0.25)(output_node) reg = ak.AutoModel(inputs=input_node,outputs=output_node, max_trials=3) Testing results after a few epochs of simple training AutoKeras also supports dealing with Multi-modal data and multi-task models, using AutoModel to customize complex model architecture. For example, the figure below contains two types of input data, and finally, the classification label and regression value have to be predicted at the same time.\nSource: Multi-Modal and Multi-Task\nAdditional Resources  AutoKeras Getting started with TensorBoard.dev AutoML 自動化機器學習  ","permalink":"https://bruceewue.github.io/blog/autokeras/","tags":["AutoML"],"title":"AutoKeras (AutoML)"},{"categories":["Projects"],"contents":"Table of Contents  Table of Contents  Project Diagram Environment Data Source Why Distributed?  Celery/RabbitMQ/Flower   Scheduling API \u0026amp; Load Test Dashboard Deployment DDNS \u0026amp; SSL CI/CD Monitoring Result Additional Resources    Project Diagram Environment Development: Ubuntu 20.04 (WSL2) + Python 3.9\nDeployment: VM (2 OCPUs (ARM64), 12GB RAM) x 2 on Oracle Cloud\nData Source Indeed job search: Source: Indeed\nI have selected Indeed, the UK\u0026rsquo;s premier job search platform, as the data source. The focus is on full-time, data science-related positions in the London area to minimise server load. Only recent listings are collected, with new posts automatically crawled daily. The system is designed for future expansion to include other platforms and job types.\nWhy Distributed?  Enhanced crawler efficiency. Single-machine limitations and failure risks when scaling up. Prevention of IP blocking from excessive crawling, mitigated with programmed sleep intervals.  Celery/RabbitMQ/Flower Celery excels as a distributed system, offering simplicity, flexibility, and reliability for processing large volumes of messages. It\u0026rsquo;s particularly effective for real-time task processing and scheduling, providing essential tools for operational management.\nIn a distributed crawling architecture, three key roles are integral:\n Producer: Responsible for dispatching tasks to the Broker. Message Broker (RabbitMQ in this context): Receives tasks and forwards them to Workers. Worker: Executes tasks received from the Broker, such as Crawlers.  The process flow is straightforward: The producer sends tasks to the queue. Workers, connected to the broker and acting as task executors, then retrieve and process these tasks according to predefined settings.\nBelow is an overview of RabbitMQ, showcasing two tasks awaiting processing in the queue. The \u0026lsquo;queue\u0026rsquo; page provides further insights into each queue\u0026rsquo;s status and detailed information about the tasks.\nWorker monitoring is efficiently handled by Flower. It allows us to check whether the workers are active and assess the status of task execution. In this project, we\u0026rsquo;ve currently enabled two workers, a number that can be adjusted based on project requirements.\nAdditionally, Flower provides a comprehensive list of tasks executed by the workers, enhancing the transparency of the workflow.\nScheduling To crawl new job vacancies daily, the Python package APScheduler schedules the function \u0026lsquo;sent_crawler_task_DE\u0026rsquo; to run every day at 12:00 Taipei time.\nscheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Taipei\u0026#34;) scheduler.add_job( id=\u0026#34;sent_crawler_task_DE\u0026#34;, func=sent_crawler_task_DE, trigger=\u0026#34;cron\u0026#34;, hour=\u0026#34;12\u0026#34;, minute=\u0026#34;0\u0026#34;, day_of_week=\u0026#34;*\u0026#34;, ) API \u0026amp; Load Test FastAPI stands out for creating RESTful APIs, with the added benefit of auto-generating OpenAPI documentation (OAS3). This feature simplifies the testing of APIs directly via a web interface.\nUpon specifying the job position and location parameters, the API adeptly returns the relevant job vacancy data in JSON format, showcasing the power of FastAPI in handling dynamic data queries.\nTo ensure the API\u0026rsquo;s robustness, conducting load testing is a crucial step. ApacheBench (ab), a Python package, is an excellent tool for this purpose. It emulates a scenario where multiple users send concurrent requests to the server, thus testing its capacity to handle heavy traffic.\nThe command below demonstrates how to perform load testing on a locally deployed API service, with a concurrency level of 10 and a total of 1000 requests:\napt-get install apache2-utils -y\rab -c 10 -n 1000 'http://127.0.0.1:8888/'\rThe resulting output provides valuable insights:\n Requests per second: Indicates how many requests the API can process each second. Time per request: Represents the average response time for each request, measured in milliseconds. Failed requests: The count of requests that didn\u0026rsquo;t succeed, offering clues about the server\u0026rsquo;s stability.  This load testing phase is integral in evaluating the performance and reliability of the API under simulated high-traffic conditions.\nDashboard A comprehensive dashboard is created using Redash, enabling easy data retrieval and visualisation. The advantage is that it is convenient to retrieve data from the DB through SQL query, and then visualize the results (currently supporting Chart, tunnel, map, pivot table, word cloud, etc.). The results are as follows.\nDeployment Since this application requires launching many services (API, scraper, rabbitmq, mysql, redash, etc.), a tool is needed to manage them uniformly. In this case, Docker Swarm is used to manage multiple services as a single virtual system, simplifying the process of deploying and managing complex applications. Docker Swarm allows you to easily scale your applications up or down by adding or removing Docker engines from the cluster, and it provides built-in load balancing and enhanced security features. (We can also use Kubernetes (K8s) to manage multiple services as a single system.)\nThrough the concept of Manager and Worker, multiple machines can be managed, and the Manager can be used to manage all services centrally, including updating services, deploying services, viewing logs, etc., making it very convenient; it is also paired with the portainer UI for management and monitoring.\nAs we can see in the picture below, there are two machines (nodes) in the cluster\nManage all services from a single interface.\nDDNS \u0026amp; SSL To ensure your API is publicly accessible, assigning a URL is essential. Commonly, this involves purchasing a domain name from registrars like domcomp, where you can explore availability and pricing.\nFor a cost-effective alternative, consider using a free Dynamic DNS (DDNS) service, such as No-IP. DDNS allows you to establish a domain name that dynamically updates to match your API\u0026rsquo;s IP address, even as it changes. Bear in mind, regular monthly renewals of the domain name are necessary to maintain its active status.\nFurther securing your API, implementing SSL (Secure Sockets Layer) is crucial. SSL certificates, like the complimentary one from Let\u0026rsquo;s Encrypt used in this project, establish a secure connection between browsers and servers. Upon successful installation, a browser displays an HTTPS symbol next to your URL, signifying a secure connection.\nThe management of SSL certificates, URL, DNS, load balancing, and their automatic renewal is streamlined through Traefik, a tool adept in reverse proxying and Docker integration.\nSource: Traefik\nBelow is the Traefik v2 dashboard, showcasing its management capabilities.\nCI/CD The integration of CI/CD (Continuous Integration/Continuous Deployment) tools is a pivotal element in modern project development. These tools automate the testing and deployment processes, substantially mitigating error risks. In this project, we\u0026rsquo;ve harnessed the capabilities of GitLab CI/CD. The Specific runners in GitLab are configured to execute CI/CD commands efficiently.\nThe CI/CD pipeline is triggered by a merge request in our setup. Once initiated, the CI process commences, encompassing automated testing, Docker image construction, and subsequent upload to Docker Hub. The deployment phase (CD) is activated upon the creation of a new tag, facilitating the service deployment to Docker Swarm.\nTools akin to GitLab CI/CD, like CircleCI and GitHub Actions, offer similar functionalities. However, GitLab\u0026rsquo;s distinct advantage lies in its open-source nature, allowing free on-premises deployment.\nThe API repository demonstrates a typical CI/CD flow: test -\u0026gt; build -\u0026gt; deploy.\nAn integral metric in CI processes is test coverage, denoting the proportion of the codebase that undergoes testing. This is crucial for ensuring code reliability and robustness.\nMonitoring Post-deployment, monitoring becomes critical to ensure that each service operates as intended and the performance of the infrastructure aligns with expectations. For this purpose, Prometheus stands out as a robust tool for collecting time-series data. It seamlessly integrates with Traefik and Node-Exporter, facilitating comprehensive data gathering.\nThe collated data is then elegantly visualized using a Grafana dashboard. This visual representation aids in understanding and analyzing the performance metrics and operational status of the services and the underlying machines.\nSource: Traefik2.2 Dashboard on Grafana\nIn our current setup, we are utilizing two machines, hence the node count is displayed as 2. This number provides a quick insight into the scale of our infrastructure being monitored.\nSource: Node Exporter for Prometheus Dashboard EN on Grafana\nResult   Data API https://bit.ly/3gGBTUX\n  Dashboard Insightful analysis of daily job postings, average salaries, and job description word clouds.\n  Additional Resources  Python 大數據專案 X 工程 X 產品 資料工程師的升級攻略 Celery - Distributed Task Queue  ","permalink":"https://bruceewue.github.io/blog/project-job-scraping-and-analysis/","tags":["Dashboard","Project","Redash"],"title":"Project - Job Scraping and Analysis"},{"categories":["Projects"],"contents":"Table of Contents  Table of Contents  Data Source Model Deployment End-to-End ML System Productionising the Model Deployment via API  Why Docker for ML?   Deploy to Cloud  Cloud Deployment Options   Monitoring  Aspects of Monitoring   Continuous Integration/Continuous Deployment (CI/CD) Additional Resources    Embarking on the journey of machine learning and data analysis, the question arises: what comes next? How can we transform our built models or analytical outcomes into practical applications? This blog post explores the creation of a machine learning system, drawing upon data from the manufacturing sector in the Tianchi Big Data Competition. It encompasses model construction, API service deployment, system monitoring, and employs a CI/CD tool for process automation.\nData Source This project deals with chemical continuous process data, aiming to predict Yield – a regression challenge. Data is sourced from AWS S3.\nDigital Manufacturing Algorithm Competition of JinNan Tianjin\nModel Deployment The actual machine learning code often forms a minor part of the overall system, with the bulk involving data handling, feature engineering, deployment, and monitoring.\nSource: Hidden Technical Debt in Machine Learning Systems(2015)\nThe machine learning modelling process includes data validation, feature engineering, model training, and deployment. A unique aspect of ML systems is their extensive reliance on Data + Models compared to traditional IT systems, posing challenges in regulating system behaviour. A critical factor is \u0026ldquo;Reproducibility,\u0026rdquo; ensuring consistent outputs from identical inputs. This necessity underscores the importance of system testing to minimise unexpected behaviour (Uncertainty).\nTo achieve \u0026ldquo;Reproducibility,\u0026rdquo; consider the following:\n Employ version control for tracking code and data modifications. Use fixed seeds for processes involving randomness. Utilise containerisation, such as Docker, for consistent application performance across different environments.  End-to-End ML System The below diagram illustrates the iterative process of constructing an end-to-end ML system.\nSource: Continuous Delivery for Machine Learning\nThis project adopts the following architecture:\nProductionising the Model Deployment involves packaging the entire pipeline, from data processing to model deployment, into a cohesive unit. We utilise sklearn.pipeline for bundling steps like feature engineering and model training. The trained model and associated configurations are then packaged and pushed to a private Package Repository on gemfury.\nDeployment via API For deployment, we utilise the FastAPI framework, supporting Python 3.6+, known for its automatic generation of OpenAPI specs and schema validation using Pydantic.\nSource: tiangolo/fastapi\nContainerisation through Docker facilitates the simultaneous launch of multiple services (MLAPI, Prometheus, Grafana, cAdvisor) using docker-compose.\nWhy Docker for ML?  Reproducibility: Ensures consistent model performance. Scalability: Broad support across major cloud platforms. Isolation: Guarantees independent operation and resource management. Maintainability: Simplifies environment setup and enhances team collaboration.  Source: SUPERCHARGE YOUR DEVELOPMENT ENVIRONMENT (USING DOCKER)\nDeploy to Cloud Deploying applications on the cloud is a versatile way to provide services. We have the choice between Platform as a Service (PaaS) and Infrastructure as a Service (IaaS). In this project, we opted for Heroku and AWS. The key difference lies in the degree of control: IaaS offers more flexibility and requires managing more components, whereas PaaS simplifies deployment.\nCloud Deployment Options   Elastic Container Registry (ECR): A vital AWS service where Docker images are stored, making them accessible to other AWS services.\n  Amazon Elastic Container Service (ECS): Manages Docker containers efficiently. The architecture necessitates setting up a cluster first, followed by defining internal services and tasks.\nSource: AWS\nFor example, in our project, a cluster named \u0026ldquo;custom-service\u0026rdquo; was established.\n  Monitoring Effective monitoring is achieved using Prometheus and Grafana. Prometheus architecture involves periodic retrieval and storage of time-series data via HTTP pull method. It supports the PromQL language for data querying and visualisation in Grafana, and also enables setting alerts for anomalies.\nSource: Prometheus\nAspects of Monitoring   Docker Monitoring: Monitors Docker system resources using google/cAdvisor.\nSource: 8bitmen.com\nMonitoring includes CPU and memory usage of services, as shown below.\n  Model Monitoring: Focuses on the predicted values of the model, including the frequency and consistency of predictions over time, and uses statistical methods for analysis.\n  Log Monitoring: Utilises the ELK Stack for monitoring textual data. Kibana is preferred over Grafana for text-based data due to performance considerations in time-series databases.\nMonitoring input data for drift is also a critical part of ensuring model accuracy.\n  Continuous Integration/Continuous Deployment (CI/CD) The goal is to automate all stages of application development, testing, and deployment. This makes the system consistently ready for release, standardises processes, and enhances transparency and traceability. In this project, CircleCI is the tool of choice.\nSource:CircleCI\nThe process begins with a \u0026ldquo;config.yml\u0026rdquo; file defining the jobs and steps. CircleCI\u0026rsquo;s integration with GitHub facilitates automated triggers for jobs upon new pull requests or commits.\nBelow is an example of the deployment process, initiated once the test_app (unit and integration tests for mlapi) passes. The model package is uploaded and released upon merging into the main branch and tagging the commit.\nAn example of the \u0026ldquo;test_app\u0026rdquo; job showcasing the use of Tox, an automated testing tool integrated with Pytest.\nAdditional Resources  Deployment of Machine Learning Models - Online Course Testing and Monitoring Machine Learning Model Deployments - Online Course  ","permalink":"https://bruceewue.github.io/blog/project-ml-model-deployment/","tags":["MLOps","AWS","Project","Prometheus","Grafana"],"title":"Project - ML Model Deployment"},{"categories":["Data Engineering"],"contents":"自己來動手製作的COVID-19 Dashboard吧，本篇將透過GCP雲端服務自動抓取最新的疫情資料，並使用Google的可視化工具Data studio串接來製作每日自動更新的疫情儀表板。\nData source CSSEGISandData/COVID-19\nDiagram 透過Cloud Scheduler定時觸發Cloud Functions中部署的爬蟲程式,並將資料儲存至Cloud Storage，並由Data Studio即時讀取最新資料並製作COVID-19即時資訊儀表板。\nGCP Source: Google\n這次採用的雲端平台GCP內含相當多的服務可以使用，目前有提供300美金3個月的免費額度可使用；而本篇使用到的服務主要有Cloud Storage, Cloud Functions, Cloud Scheduler\nCloud Functions 利用Cloud Functions來新增一個可透過Http request觸發的爬蟲程式，記得在requirements.txt 寫入需安裝的套件(此處為pandas以及google-cloud-storage)，\n爬取確診人數的部分如下，也可以新增迴圈連同死亡、復原的人數一起抓取。\nimport pandas as pd def crawler(request): # get data from github df = pd.read_csv(\u0026#34;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026#34;) # wide format to long format df = pd.melt(df, id_vars=df.columns[:4], value_vars=df.columns[4:], var_name=\u0026#34;date\u0026#34;, value_name=\u0026#34;count\u0026#34;) return df.to_csv(index=False) 由於需要抓取自動後上傳至Cloud Storage，所以還需加入以下網址內程式碼 Uploading objects\nNote:\n Cloud Functions只允許在 /tmp路徑下才能存檔。 Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Storage 雲端儲存空間，新增bucket後即可開始使用，也能像google drive一樣手動上傳檔案，但Cloud Storage的優勢是更能方便地在GCP上與各服務串接；免費的部分有5GB的額度。\n觸發Cloud functions後可以確認看看是否有確實上傳確診、死亡、復原的資料 Note:\n Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Scheduler 可以依據原始資料的更新時間來設定希望多久執行一次爬蟲，設定方式是像在Linux中的Crontab排程，使用Cron語法，可以利用這個網站確認自己撰寫的排程是否符合預期。\n設定完成的狀態如下 Note:\n Cloud Scheduler完全免費方案只有設定3個Job的額度  Data Studio 在設定好資料後就可以嘗試來把它視覺化了，這裡採用的是google免費的Data Studio，算是滿容易上手的工具，跟GCP串接當然也是沒有問題的。\n此處新增資料源-\u0026gt;從Data Storage內選取三份csv檔案\nResult COVID-19 Dashboard Animated Bar Chart  即時資訊互動儀表板如下，可自由選擇Country/Region (建議可點擊右下角\u0026quot;Google數據分析\u0026quot;觀看完整報表)  \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rAdditional Resources  打造動態報表！雲端 Python 爬蟲資料流  ","permalink":"https://bruceewue.github.io/blog/covid_dashboard/","tags":["Data studio","GCP"],"title":"Create COVID-19 Dashboard by Data studio"},{"categories":["Others"],"contents":"Markdown Example\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n  Steve Francia test\n  List  有序1  四個空格內縮   有序2 有序3  Unordered List  無序1  四個空格內縮   無序2  Blockquote  This is a blockquote example.\n Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) No language indicated, so no syntax highlighting. But let's throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;.\rInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nTables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    Youtube embeded   ","permalink":"https://bruceewue.github.io/blog/markdown_test/","tags":["Hugo"],"title":"Markdown test"}]