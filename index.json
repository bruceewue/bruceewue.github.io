[{"categories":["Data Engineering"],"contents":"This post demonstrates how to scrape live bitcoin price data using these tools: Airflow, MongoDB, and Kafka. The data is then saved to a NoSQL database (MongoDB) and periodically processed (ETL) to change the price unit from USD to GBP.\nProject Diagram Environment Ubuntu 20.04 (WSL2) + python 3.8\nData source Source: coincap\nUtilize its API(coincap_api) to scrape real-time (live) bitcoin data.\nMongoDB NoSQL(aka \u0026ldquo;not only SQL\u0026rdquo;), which is different from relational databases, is used here due to the necessity to process high-speed and mass-produced data. NoSQL databases are designed for unstructured data and ideal for big data applications(schema-free).\nSource:MongoDB\nMapping relational database to MongoDB Source:RDBMS_MongoDB\nMongoDB stores data in BSON format(Binary JSON)， \u0026ldquo;_id\u0026rdquo; is an OBJECT not STRING. To prevent errors when extracting data from the database, we can convert it to STRING.\nWe can use Studio3T (free MongoDB GUI) to connect to our MongoDB database.\nUse python to connect to mongoDB import pymongo client = pymongo.MongoClient(\u0026#39;localhost\u0026#39;, 27017) #load data from mongoDB to Pandas database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name query = {} records = pd.DataFrame(list(collection.find(query))) # insert data into mongoDB df = pd.DataFrame({\u0026#39;price\u0026#39;: [\u0026#39;12.4\u0026#39;, \u0026#39;1.4\u0026#39;, \u0026#39;2.6\u0026#39;]}) records = df.to_dict(\u0026#39;records\u0026#39;) collection.insert_many(records) Kafka Apache Kafka is a popular open-source distributed event streaming platform designed to handle continuous streams of events in real-time. Because of the bottlenecks that might be caused when large amounts of data continue to be stored directly in the DB, we can consider using Kafka here.\nSource:Kafka\nThree steps to implement for event streaming:\n To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. To store streams of events durably and reliably for as long as you want. To process streams of events as they occur or retrospectively.  First, we need to start both Zookeper and Kafka services.\nbin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties Open a terminal, create a new topic named \u0026ldquo;kafka-topics\u0026rdquo;\nbin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1  Producers: client applications that publish (write) events to Kafka consumers: those that subscribe to (read and process) these events  # producer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; producer = KafkaProducer(bootstrap_servers=[brokers]) url = \u0026#34;https://api.coincap.io/v2/assets/bitcoin\u0026#34; while True: r = requests.get(url) v = json.dumps(r.json()).encode() k = datetime.now().strftime(\u0026#34;%Y-%m-%d%H:%M:%S:%f\u0026#34;).encode() producer.send(topic, key=k, value=v) # time.sleep(5) The following section is consumer, which receives the data and stores it in the DB\n# consumer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; consumer = KafkaConsumer(topic, bootstrap_servers=[brokers]) client = pymongo.MongoClient(\u0026#34;localhost\u0026#34;, 27017) database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name for msg in consumer: dict_data = json.loads(msg.value) dict_data[\u0026#34;data\u0026#34;][\u0026#34;timestamp\u0026#34;] = msg.key.decode() collection.insert_many([dict_data[\u0026#34;data\u0026#34;]]) Airflow Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. I\u0026rsquo;d like to automatically convert the price of bitcoin from USD to GBP and then store them in the DB.\nBefore using Airflow, we need to set up the connection to the DB(MongoDB) via the Airflow Web UI. (default: pymongo)\nWe need to add the folder \u0026ldquo;dags\u0026rdquo; under the airflow folder and then add a python file (.py) in the folder to define the three tasks of ETL in the DAG, namely Extract, Transform, and Load. They are expected to be executed sequentially at 00:30 every day. Here\u0026rsquo;s a quick, simple, and free editor for cron schedule crontab.guru, which can help us set the schedule_interval.\n dags/dag.py   task1 -\u0026gt; task2 -\u0026gt; task3 Log Result Additional Resources  Kafka Airflow  ","permalink":"https://bruceewue.github.io/blog/etl/","tags":["Airflow","Kafka","MongoDB"],"title":"Data pipeline (cryptocurrency)"},{"categories":["Machine Learning"],"contents":"Greater model parameters and network design are always preferred for better performance during model training. Manually tuning these parameters and architecture can be difficult and time-consuming, so in this post, I\u0026rsquo;ll attempt to use an AutoML tool called Autokeras to accelerate this process.\nAutoKeras AutoKeras is an open-source AutoML tool based on Keras that automates designing and tuning machine learning models. It allows users to quickly and easily train high-performance models without extensive expertise in machine learning. By using Autokeras, we can save time and effort compared to manual tuning and train high-quality models for a variety of tasks.\nAutoKeras uses Efficient Neural Architecture Search (ENAS) to automate the process of designing and tuning machine learning models. It also includes support for a variety of preprocessing methods, such as normalization, vectorization, and encoding. AutoKeras can be used for a wide range of tasks, including structured data, image data, time series prediction, and multitasking.\nSource: AutoKeras\nThe official website of AutoKeras provides examples that show how to use the tool in just a few lines of code.\nimport autokeras as ak # Initialize the image classifier. clf = ak.ImageClassifier() # Feed the image classifier with training data. clf.fit(x_train, y_train) # Predict with the trained model. predicted_y = clf.predict(x_test) Environment With Colab Pro, some Python modules and GPU are already installed and configured in Colab so that we can focus on coding.\nExample 1 - Fraud detection The first case is a credit card fraud detection on Kaggle, which is a structured data classification problem\ndataset: Kaggle-Data-Credit-Card-Fraud-Detection (284807 rows × 31 columns)\nThe StructuredDataClassifier is used here to handle the structured data classification problem.\n# StructuredDataClassifier cbs = [ # Switch to the next model after three consecutive training cycles without improvement # monitor default: val_loss tf.keras.callbacks.EarlyStopping(patience=3,monitor=\u0026#39;val_loss\u0026#39;), # histogram_freq=1 means to count once in each training cycle, and display the weight and bias distribution of each layer of neurons during training in a histogram tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1) ] # max_trials is the number of models to try, the default value is 100, you can find the structure of the default model in oracle.json in the directory clf=ak.StructuredDataClassifier(max_trials=3) # The default value of epoch is 1000, and it may end early when using earlystopping history=clf.fit(x_train,y_train,callbacks=cbs,epochs=25) In addition to directly using StructuredDataClassifier, if you need to customize the model architecture, search space, etc., you can also use AutoModel to further design the model, as shown below.\ninput_node = ak.StructuredDataInput() output_node = ak.StructuredDataBlock()(input_node) output_node = ak.ClassificationHead()(output_node) clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3) Save and load models:\n# Save model try: # TensorFlow SavedModel format model.save(\u0026#39;model_autokeras\u0026#39;,save_format=\u0026#39;tf\u0026#39;) except: # keras h5 format model.save(\u0026#39;model_autokeras.h5\u0026#39;) # Load AutoKeras model from tensorflow.keras.models import load_model loaded_model = load_model(\u0026#39;./auto_model/best_model\u0026#39;, custom_objects=ak.CUSTOM_OBJECTS) Training process (Colab)  If the training is interrupted and executed again, it will continue the training based on the records After the training is completed, AutoKeras will use all training set + validation set for training again with the best model. After all trials are completed, the model with the best performance will be automatically stored in best_model/* Due to the problem of category imbalance in this data set, Autokeras also supports setting class_weight in fit() for weighting the loss function, so that the model will pay more attention to minority categories.  Training Visualization - TensorBoard To understand the training process, you can use tensorboard to read the log data stored in callback to visualize the training process; use the tensorboard.dev to generate a public page for sharing with others.\nimport tensorflow as tf import datetime from tensorboard.plugins.hparams import api as hp !tensorboard dev upload --logdir logs \\ --name \u0026#34;Simple experiment\u0026#34; \\ --description \u0026#34;fraud detect\u0026#34; \\ Execute the above program fragment and check \u0026ldquo;Reload data\u0026rdquo; before training, and the data will be updated during training.\nAccuracy and loss changes during training Model architecture ClearML If you want to monitor and track the results and share the experiments with people, you can also use the MLOps platform ClearML that supports AutoKeras to present it on an interactive web page.\nfrom clearML import Task # register at https://app.community.clear.ml # get the \u0026#39;key\u0026#39; and \u0026#39;secret\u0026#39;： Task.set_credentials( api_host=\u0026#39;https://api.community.clear.ml\u0026#39;, web_host=\u0026#39;https://app.community.clear.ml\u0026#39;, files_host=\u0026#39;https://files.community.clear.ml\u0026#39;, key=\u0026#39;xxxxx\u0026#39;, secret=\u0026#39;xxxxxx\u0026#39; ) # Create a new ClearML task # Visit https://app.community.clear.ml/dashboard task = Task.init(project_name=\u0026#39;autokeras age\u0026#39;, task_name=\u0026#39;age regressor\u0026#39;) The project overview is on the ClearML main page In addition to the loss and accuracy during training, it also includes the machine\u0026rsquo;s status. Changes in weight and bias in the model  ClearML will also show the results through the library matplotlib and seaborn  Example 2 - Age prediction This example is about age prediction, which is a regression problem to predict age from photos.\n(Dataset: wiki_crop)\nAfter image filtering and pre-processing, the images were converted to 52984 128*128 images.\nTraining data: Use AutoModel to customize model details, which is similar to Keras functional API. AutoKeras then finds the best result from the conditions we give.\ninput_node = ak.ImageInput() # If just want to try EfficientNet b0~b7 output_node = ak.ImageBlock(block_type=\u0026#39;efficient\u0026#39;, normalize=True, augment=True)(input_node) output_node = ak.RegressionHead(droupout=0.25)(output_node) reg = ak.AutoModel(inputs=input_node,outputs=output_node, max_trials=3) Testing results after a few epochs of simple training AutoKeras also supports dealing with Multi-modal data and multi-task models, using AutoModel to customize complex model architecture. For example, the figure below contains two types of input data, and finally, the classification label and regression value have to be predicted at the same time.\nSource: Multi-Modal and Multi-Task\nAdditional Resources  AutoKeras Getting started with TensorBoard.dev AutoML 自動化機器學習  ","permalink":"https://bruceewue.github.io/blog/autokeras/","tags":["AutoML"],"title":"AutoKeras (AutoML)"},{"categories":["Data Engineering"],"contents":"Since I\u0026rsquo;d like to know the situation of local data-related vacancies in London, England, I have built a job vacancy analysis system, including a distributed crawler with automatic scheduling, a job vacancy analysis dashboard, and a data API. The results are shown at the end of this post.\nProject Diagram Environment Development: Ubuntu 20.04(WSL2) + python 3.9\nDeployment: VM(2 OCPUs(ARM64), 12GB RAM) x 2 in Oracle Cloud\nData source Indeed job search: Source: Indeed\nCurrently, I choose the UK\u0026rsquo;s largest job search platform, \u0026ldquo;Indeed\u0026rdquo; as the data source. Only data science-related and full-time positions in the London area are crawled to avoid burdening the server. Only recent data is collected, new posts added daily are automatically crawled in the daily schedule, and have the flexibility to add other platforms or other types of vacancies in the future.\nWhy distributed?  Increase the efficiency of the crawler. When the number of crawlers increases, a single computer may be unable to handle it, and if the machine suddenly fails, the crawler will be interrupted. Avoid IP blocking due to excessive crawling (used with sleep in the program).  Celery/RabbitMQ/Flower Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages while providing operations with the tools required to maintain such a system.\nIt’s a task queue focusing on real-time processing and supporting task scheduling.\nThere are three roles in a distributed crawling architecture:\n Producer, which dispatches tasks to the Broker Message Broker, which receives tasks and forwards them to Workers (in this post, I use RabbitMQ as the distributed task forwarding/message delivery center) Worker, which receives tasks from the Broker and executes them (e.g., Crawler)  Process: the producer sends a task to the queue. After the workers (the units that execute tasks provided by Celery) connect to the broker, they can receive tasks from the queue and process them based on the settings.\nThe following figure shows the RabbitMQ overview. We can see that two tasks are waiting for workers in the queue. The page \u0026lsquo;queue\u0026rsquo; also provides information about each queue\u0026rsquo;s status and the tasks\u0026rsquo; details.\nWe can monitor the status of the workers through Flower to see if it is on and the status of the task execution. Currently, only two workers are enabled for this project, which can be increased or decreased according to the needs.\nThe list of tasks executed by the workers. Scheduling Because we need to automatically crawl the new job vacancies of the day, the python package APScheduler is used here to execute tasks. The following code snippet is to set a timer and execute the function \u0026lsquo;sent_crawler_task_DE\u0026rsquo; every day at 12:00 Taipei time from Monday to Sunday\nscheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Taipei\u0026#34;) scheduler.add_job( id=\u0026#34;sent_crawler_task_DE\u0026#34;, func=sent_crawler_task_DE, trigger=\u0026#34;cron\u0026#34;, hour=\u0026#34;12\u0026#34;, minute=\u0026#34;0\u0026#34;, day_of_week=\u0026#34;*\u0026#34;, ) API \u0026amp; Load test FastAPI can be used to create RESTful APIs and automatically generate the OpenAPI file (OAS3), allowing us to test the APIs on the web.\nAfter inputting the position and location parameters, the API will return job vacancy data in JSON format.\nNext, we can try performing load testing on the created API. The ApacheBench (ab) python package can be used for this purpose. It simulates multiple users sending multiple requests to the server simultaneously.\nThe following command can be used to perform load testing on the API service deployed on the local machine (concurrency=10, requests=1000):\napt-get install apache2-utils -y\rab -c 10 -n 1000 'http://127.0.0.1:8888/'\rHere is an example of the output:\nEquest per second: the number of requests that the API can handle per second.\nTime per request: the average time spent on each request (in milliseconds).\nFailed requests: the number of failed requests. This may be related to the stability of the server.\nDashboard In order to have a clear understanding of job vacancies, a dashboard is produced using Redash, an open-source BI system. The advantage is that it is convenient to retrieve data from the DB through SQL query, and then visualize the results (currently supporting Chart, tunnel, map, pivot table, word cloud, etc.). The results are as follows.\nThe following is an analysis of the number of jobs posted daily for each job type, the average salary of each job type, and the word cloud of the JD in the job postings.\nDeployment Since this application requires launching many services (API, scraper, rabbitmq, mysql, redash, etc.), a tool is needed to manage them uniformly. In this case, Docker Swarm is used to manage multiple services as a single virtual system, simplifying the process of deploying and managing complex applications. Docker Swarm allows you to easily scale your applications up or down by adding or removing Docker engines from the cluster, and it provides built-in load balancing and enhanced security features. (We can also use Kubernetes (K8s) to manage multiple services as a single system.)\nThrough the concept of Manager and Worker, multiple machines can be managed, and the Manager can be used to manage all services centrally, including updating services, deploying services, viewing logs, etc., making it very convenient; it is also paired with the portainer UI for management and monitoring.\nAs we can see in the picture below, there are two machines (nodes) in the cluster\nManage all of our services from a single interface.\nDDNS \u0026amp; SSL In order to make your API available to the public, you will need to provide a URL rather than just an IP address. Typically, domain names are purchased from a domain name registrar and come at a cost. You can check the availability and pricing of a domain name by going to a domain name registrar such as domcomp and searching for the domain you are interested in.\nAlternatively, you can use a free dynamic DNS (DDNS) service such as No-IP to provide a URL for your API. With a DDNS service, you can create a domain name that automatically points to your API\u0026rsquo;s IP address, even if it changes. However, you will need to manually renew the domain name each month to keep it active.\nSSL (Secure Sockets Layer) is a security certificate that enables a secure connection between a web browser or computer and a server. After successful installation, an HTTPS symbol will appear next to the URL in the browser. Many organizations issue SSL certificates, and this project uses a free SSL certificate from Let\u0026rsquo;s Encrypt. The certificate is managed using the tool Traefik, which performs reverse proxy and includes features for managing URLs, DNS, load balancing, and automatically renewing SSL certificates. It is also easy to integrate with Docker.\nSource:Traefik\nThe dashboard for Traefik v2.\nCI/CD During the development and iteration of the project, a CI/CD (Continuous Integration/Continuous Deployment) tool can be used to automate the testing and deployment process, reducing the chance of errors. This project uses Gitlab CI/CD, and the Specific runners execute CI/CD commands. Currently, the configuration is set to trigger CI only when a merge request is made. The CI process automatically performs testing, builds a docker image, and uploads it to docker hub. After a new tag is created, the CD process is triggered, and the service is deployed to the docker swarm. Similar CICD tools include CircleCI, GitHub Actions, etc. The advantage of Gitlab is that it is open source and can be deployed on your own server (on-premises) for free.\nFor example, in the API repository, the process would be test -\u0026gt; build -\u0026gt; deploy.\nTest coverage refers to the percentage of your codebase that is executed during testing in a CI process. Monitoring After deployment, it is important to monitor the service to ensure that the operation status of each service and the performance of the machines are as expected. Prometheus can be used to collect time-series data through integration with traefik and node-exporter, and the results can be visualized in a Grafana dashboard.\nSource:Traefik2.2\nIn this case, there are two machines being used, so the node count is 2. Source:Node Exporter for Prometheus Dashboard EN\nResult  Data API  https://bit.ly/3gGBTUX\n Dashboard  https://bit.ly/3oQywix \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rNote: The data up to May 2022 is used as an example case.\nAdditional Resources  Python 大數據專案 X 工程 X 產品 資料工程師的升級攻略 Celery - Distributed Task Queue  ","permalink":"https://bruceewue.github.io/blog/job_scrape/","tags":["Dashboard","Project","Redash"],"title":"Project - Job Scraping and Analysis"},{"categories":["Machine Learning"],"contents":"在學習了眾多機器學習、資料分析等方法後不禁思考:下一步呢? 該如何讓建立的模型或分析的成果能實際落地提供使用?\n本篇以天池競賽中的製造業數據為例，創建機器學習系統，包含模型建置、API服務部署及系統監控，並結合CICD將流程自動化。\nData source 本數據集是關於化工連續製程的數據，目標是希望預測Yield，屬於迴歸問題(regression)，本案例中模擬從AWS S3取得資料\nDigital Manufacturing Algorithm Competition of JinNan Tianjin\nModel Deployment 要建立一個機器學習系統，ML code可能只佔相當小的一部份，大多數會是在處理處理資料蒐集、特徵工程、部署、系統監控等等 Source:Hidden Technical Debt in Machine Learning Systems(2015)\n以機器學習建模來說，一般會經過Data validation-\u0026gt;Feature engineering-\u0026gt;Model training-\u0026gt;Model deployment。機器學習系統比起傳統IT系統多了Data+Model，以致較難以明確規範系統行為；其中要注意的是\u0026rdquo;Reproducibility\u0026rdquo;, 意思是在上述各個流程模組中，必須確保當給定一樣的Input就應該得到一樣的Output，這是ML系統所面臨的重要挑戰；另外也是必須針對系統撰寫測試(Testing)的重要原因，期望能降低系統發生非預期行為(Uncertainty)的風險。\n那該如何確保\u0026rdquo;Reproducibility\u0026rdquo;? 基本有三個方向\n 版本控制 (Version Control) 盡可能設置Seed (if require randomness) 使用容器 (Container)  End-to-end ML system 下圖為建構端到端ML系統的流程，並且可能會不斷地迭代。 Source: Continuous Delivery for Machine Learning end-to-end process\n下圖是在學習機器學習部署的過程所歸納整理，本案例也參考這樣的設計來構建系統 以下針對幾點說明\nProductionize Model 其實我們要部署的並非只有模型本身，而是整個從資料處理到建模部署的pipeline。在這部分會將特徵工程、建模等一系列步驟透過sklearn.pipeline進行封裝，將訓練完的模型(.pkl)給予版本並將前處理、模型訓練、設定檔等打包成package並推送至Python package index server,常見的有公開的PyPI,而此處是使用gemfury,為的是能夠建立不公開的Package Repository\nDeployment(Sering the model via API) 在部署的部分，此處採用Python 3.6+之支援異步高效能框架FastAPI，好處是可以自動生成openAPI sepc並且可以簡單的直接使用Pydantic進行Schema Validation。\nSource: tiangolo/fastapi\n下圖是本例的API docs 在應用的部署上會透過Docker將服務容器化，由於需要運作多個服務(MLAPI, Prometheus, Grafana, cAdvisor)，此處會透過docker-compose一次啟動\nWhy docker for ML?  重現性(Reproducibility): 以確保每次運行結果一致 可擴展性(Scalibility): 各大雲端皆支援，易於整合 隔離性(Isolation): 確保服務獨立運作且資源隔離(Process Isolation) 維護性(Maintainability): 環境易設定且可攜帶使團隊間容易分享 Source: SUPERCHARGE YOUR DEVELOPMENT ENVIRONMENT (USING DOCKER)  Deploy to Cloud 可以將應用部署上雲端以提供服務，可以選擇是PaaS或是IaaS，此處各選擇Heroku 以及 AWS 來部署，PaaS 或是 IaaS的差異如下圖，選擇IaaS的話有較多部份需要自己管理，但也相對較為彈性。 Source: RedHat\n此處以AWS部署為例，使用到的服務有\n  Elastic Container Registry(ECR): 將打包好的image上傳到此存放，供其他AWS服務存取   Amazon Elastic Container Service(ECS):AWS上管理Dokcer container的服務，下圖為ECS的架構，需先創建一個Cluster在定義內部的Service與Task\nSource: AWS\n  下圖為本案例實作時所創建的Cluster，並且創建了一個名為custom-service的服務 Monitoring 透過Prometheus與Grafana的搭配來搭建監控儀表板。下圖為Prometheus架構，透過HTTP的PULL方式週期性抓取並儲存時間序列資料，只要提供對應的HTTP介面並且符合 Promethues 定義的格式即可監控；且有提供PromQL語言進行查詢再透過Grafana進行資料可視化，當異常發生時也可以設置alert警報。\nSource: Prometheus\n本案例在監控的部分主要分三個部分:\n Docker Monitoring: 用於監控Docker系統資源，使用工具:google/cAdvisor Source: 8bitmen.com  下圖為案例示範，主要針對四個服務的CPU跟記憶體進行監控 Model Monitoring: 可以針對模型的預測值進行監控  每單位時間有多少預測值，是否發生異常 監控模型的預測值在不同時間點是否發生顯著變化(可搭配Z-score/SEM/STD等統計資訊)   Log  可以用ELK Stack進行    如果有text類型資料，Kibana相對Grafana適合，時間序列DB可能因high cardinality造成performance的問題\nContinuous Integration/Continuous Deployment(CI/CD) 主要是希望能將應用開發測試與部署的每個階段自動化，可讓系統是在一個\u0026quot;always releasable\u0026quot;的狀態，且整個過程標準化、透明化並易於追蹤。CI/CD的工具相當多，此處使用的工具是CircleCI Source:CircleCI\n首先須撰寫設定檔config.yml，透過定義各種job跟steps來定義流程中的每個步驟，包含所需的環境或設定，所要執行的指令等等。CircleCI可以方便地與github專案連動，當新開一個Pull request，或後續該分支的commit時，就會觸發jobs。\n下圖為本案範例，當test_app(mlapi的單元測試與整合測試)通過後，才進行應用的部署，並且只有在merger到main並給予該commit tag後才進行model package的上傳發佈。\n下圖以\u0026quot;test_app\u0026quot;這個job為例，其中tox是一款python自動化測試工具，可以整合pytest一起使用。 Additional Resources  Deployment of Machine Learning Models - Online Course Testing and Monitoring Machine Learning Model Deployments - Online Course  ","permalink":"https://bruceewue.github.io/blog/ml_deployment/","tags":["MLOps","AWS","Project","Prometheus","Grafana"],"title":"Project - ML Model Deployment"},{"categories":["Data Engineering"],"contents":"自己來動手製作的COVID-19 Dashboard吧，本篇將透過GCP雲端服務自動抓取最新的疫情資料，並使用Google的可視化工具Data studio串接來製作每日自動更新的疫情儀表板。\nData source CSSEGISandData/COVID-19\nDiagram 透過Cloud Scheduler定時觸發Cloud Functions中部署的爬蟲程式,並將資料儲存至Cloud Storage，並由Data Studio即時讀取最新資料並製作COVID-19即時資訊儀表板。\nGCP Source: Google\n這次採用的雲端平台GCP內含相當多的服務可以使用，目前有提供300美金3個月的免費額度可使用；而本篇使用到的服務主要有Cloud Storage, Cloud Functions, Cloud Scheduler\nCloud Functions 利用Cloud Functions來新增一個可透過Http request觸發的爬蟲程式，記得在requirements.txt 寫入需安裝的套件(此處為pandas以及google-cloud-storage)，\n爬取確診人數的部分如下，也可以新增迴圈連同死亡、復原的人數一起抓取。\nimport pandas as pd def crawler(request): # get data from github df = pd.read_csv(\u0026#34;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026#34;) # wide format to long format df = pd.melt(df, id_vars=df.columns[:4], value_vars=df.columns[4:], var_name=\u0026#34;date\u0026#34;, value_name=\u0026#34;count\u0026#34;) return df.to_csv(index=False) 由於需要抓取自動後上傳至Cloud Storage，所以還需加入以下網址內程式碼 Uploading objects\nNote:\n Cloud Functions只允許在 /tmp路徑下才能存檔。 Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Storage 雲端儲存空間，新增bucket後即可開始使用，也能像google drive一樣手動上傳檔案，但Cloud Storage的優勢是更能方便地在GCP上與各服務串接；免費的部分有5GB的額度。\n觸發Cloud functions後可以確認看看是否有確實上傳確診、死亡、復原的資料 Note:\n Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Scheduler 可以依據原始資料的更新時間來設定希望多久執行一次爬蟲，設定方式是像在Linux中的Crontab排程，使用Cron語法，可以利用這個網站確認自己撰寫的排程是否符合預期。\n設定完成的狀態如下 Note:\n Cloud Scheduler完全免費方案只有設定3個Job的額度  Data Studio 在設定好資料後就可以嘗試來把它視覺化了，這裡採用的是google免費的Data Studio，算是滿容易上手的工具，跟GCP串接當然也是沒有問題的。\n此處新增資料源-\u0026gt;從Data Storage內選取三份csv檔案\nResult COVID-19 Dashboard Animated Bar Chart  即時資訊互動儀表板如下，可自由選擇Country/Region (建議可點擊右下角\u0026quot;Google數據分析\u0026quot;觀看完整報表)  \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rAdditional Resources  打造動態報表！雲端 Python 爬蟲資料流  ","permalink":"https://bruceewue.github.io/blog/covid_dashboard/","tags":["Data studio","GCP"],"title":"Create COVID-19 Dashboard by Data studio"},{"categories":["Others"],"contents":"Markdown Example\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n  Steve Francia test\n  List  有序1  四個空格內縮   有序2 有序3  Unordered List  無序1  四個空格內縮   無序2  Blockquote  This is a blockquote example.\n Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) No language indicated, so no syntax highlighting. But let's throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;.\rInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nTables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    Youtube embeded   ","permalink":"https://bruceewue.github.io/blog/markdown_test/","tags":["Hugo"],"title":"Markdown test"}]