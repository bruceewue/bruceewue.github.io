[{"categories":["Data"],"contents":"This post demonstrates how to scrape live bitcoin price data using these tools: Airflow, MongoDB, and Kafka. The data is then saved to a NoSQL database (MongoDB) and periodically processed (ETL) to change the price unit from USD to GBP.\nProject Diagram Environment Ubuntu 20.04 (WSL2) + python 3.8\nData source Source: coincap\nUtilize its API(coincap_api) to scrape real-time (live) bitcoin data.\nMongoDB NoSQL(aka \u0026ldquo;not only SQL\u0026rdquo;), which is different from relational databases, is used here due to the necessity to process high-speed and mass-produced data. NoSQL databases are designed for unstructured data and ideal for big data applications(schema-free).\nSource:MongoDB\nMapping relational database to MongoDB Source:RDBMS_MongoDB\nMongoDB stores data in BSON format(Binary JSON)， \u0026ldquo;_id\u0026rdquo; is an OBJECT not STRING. To prevent errors when extracting data from the database, we can convert it to STRING.\nWe can use Studio3T (free MongoDB GUI) to connect to our MongoDB database.\nUse python to connect to mongoDB import pymongo client = pymongo.MongoClient(\u0026#39;localhost\u0026#39;, 27017) #load data from mongoDB to Pandas database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name query = {} records = pd.DataFrame(list(collection.find(query))) # insert data into mongoDB df = pd.DataFrame({\u0026#39;price\u0026#39;: [\u0026#39;12.4\u0026#39;, \u0026#39;1.4\u0026#39;, \u0026#39;2.6\u0026#39;]}) records = df.to_dict(\u0026#39;records\u0026#39;) collection.insert_many(records) Kafka Apache Kafka is a popular open-source distributed event streaming platform designed to handle continuous streams of events in real-time. Because of the bottlenecks that might be caused when large amounts of data continue to be stored directly in the DB, we can consider using Kafka here.\nSource:Kafka\nThree steps to implement for event streaming:\n To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. To store streams of events durably and reliably for as long as you want. To process streams of events as they occur or retrospectively.  First, we need to start both Zookeper and Kafka services.\nbin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties Open a terminal, create a new topic named \u0026ldquo;kafka-topics\u0026rdquo;\nbin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1  Producers: client applications that publish (write) events to Kafka consumers: those that subscribe to (read and process) these events  # producer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; producer = KafkaProducer(bootstrap_servers=[brokers]) url = \u0026#34;https://api.coincap.io/v2/assets/bitcoin\u0026#34; while True: r = requests.get(url) v = json.dumps(r.json()).encode() k = datetime.now().strftime(\u0026#34;%Y-%m-%d%H:%M:%S:%f\u0026#34;).encode() producer.send(topic, key=k, value=v) # time.sleep(5) The following section is consumer, which receives the data and stores it in the DB\n# consumer.py brokers, topic = \u0026#34;localhost:9092\u0026#34;, \u0026#34;coincap\u0026#34; consumer = KafkaConsumer(topic, bootstrap_servers=[brokers]) client = pymongo.MongoClient(\u0026#34;localhost\u0026#34;, 27017) database = client[\u0026#34;db_test\u0026#34;] # Database Name collection = database[\u0026#34;coincap\u0026#34;] # Table Name for msg in consumer: dict_data = json.loads(msg.value) dict_data[\u0026#34;data\u0026#34;][\u0026#34;timestamp\u0026#34;] = msg.key.decode() collection.insert_many([dict_data[\u0026#34;data\u0026#34;]]) Airflow Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. I\u0026rsquo;d like to automatically convert the price of bitcoin from USD to GBP and then store them in the DB.\nBefore using Airflow, we need to set up the connection to the DB(MongoDB) via the Airflow Web UI. (default: pymongo)\nWe need to add the folder \u0026ldquo;dags\u0026rdquo; under the airflow folder and then add a python file (.py) in the folder to define the three tasks of ETL in the DAG, namely Extract, Transform, and Load. They are expected to be executed sequentially at 00:30 every day. Here\u0026rsquo;s a quick, simple, and free editor for cron schedule crontab.guru, which can help us set the schedule_interval.\n dags/dag.py   task1 -\u0026gt; task2 -\u0026gt; task3 Log Result Additional Resources  Kafka Airflow  ","permalink":"https://bruceewue.github.io/blog/etl/","tags":["Data engineering","Python"],"title":"Data pipeline (cryptocurrency)"},{"categories":["Machine Learning"],"contents":"在模型訓練的時候總是希望能得到較佳的模型參數與網路架構，如果仰賴自行調校可能會相對困難與耗時，所以希望透過AutoML工具來替我們完成。\nAutoKeras 本篇嘗試使用基於Keras的AutoML框架名為AutoKeras(使用超參數搜尋和ENAS(Efficient Neural Architecture Search))，也可以協助完成一些前處理手段(normalization,vectorization,encoding,..)。舉凡結構化資料，圖像資料，時間序列預測，多任務等都支援。\nSource: AutoKeras\n在官網範例中只要三行就可以完成訓練和預測\nimport autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test) Environment 搭配Colab pro進行訓練，其基本模組與GPU已安裝並配置好，可以直接專注於程式碼開發相當方便，升級到pro之後目前較常分配到的顯卡是P100。\nExample 1 - Fraud detection 案例是Kaggle上的信用卡詐欺偵測，屬於結構化資料的分類問題 dataset: Kaggle-Data-Credit-Card-Fraud-Detection (284807 rows × 31 columns)\n使用StructuredDataClassifier處理結構資料分類問題。\ncbs = [ # 連續三個周期訓練沒進步就換到下個模型 # monitor default: val_loss tf.keras.callbacks.EarlyStopping(patience=3,monitor=\u0026#39;val_loss\u0026#39;), # histogram_freq=1表示在每次訓練週期各統計一次，並以直方圖展示訓練時各層神經元weight,bias分布 tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1) ] # max_trials為嘗試模型的數量，預設值為100，可以在目錄下的oracle.json找到預設模型的的結構 clf=ak.StructuredDataClassifier(max_trials=3) # epoch 預設值1000，結合earlystopping可能提前結束 history=clf.fit(x_train,y_train,callbacks=cbs,epochs=25) 除了直接使用StructuredDataClassifier之外，如需自訂模型架構、搜尋空間等也可使用AutoModel對模型做進一步設計，如下所示。\ninput_node = ak.StructuredDataInput() output_node = ak.StructuredDataBlock()(input_node) output_node = ak.ClassificationHead()(output_node) clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3) 儲存與載入模型\n# Save model try: # TensorFlow SavedModel format model.save(\u0026#39;model_autokeras\u0026#39;,save_format=\u0026#39;tf\u0026#39;) except: # keras h5 format model.save(\u0026#39;model_autokeras.h5\u0026#39;) # Load AutoKeras model from tensorflow.keras.models import load_model loaded_model = load_model(\u0026#39;./auto_model/best_model\u0026#39;, custom_objects=ak.CUSTOM_OBJECTS) 訓練過程 (Colab)  如果中斷訓練後再次執行，會依據已經紀錄的內容來接續訓練 訓練完成後，AutoKeras會針對表現最好的模型，合併使用所有training set + validation set再完整的參與訓練一次。 所有trial完成後，表現最佳的模型會自動存放在best_model/* 由於本資料集存在類別不均衡的問題(imbalanced)，Autokeras也支援在fit()中設定class_weight來調整各類別的損失值(loss)比重，使模型更重視少數類別。  Training Visualization - TensorBoard 為了瞭解訓練過程，可以使用tensorboard讀取在callbck中儲存的log資料將訓練過程視覺化；利用免費的tensorboard.dev則可以產生公開的頁面方便與他人分享。\nimport tensorflow as tf import datetime from tensorboard.plugins.hparams import api as hp !tensorboard dev upload --logdir logs \\ --name \u0026#34;Simple experiment\u0026#34; \\ --description \u0026#34;fraud detect\u0026#34; \\ 在訓練前執行以上程式片段並勾選\u0026quot;Reload data\u0026rdquo;，訓練時數據就會隨著更新。\n訓練過程的accuracy與loss變化 模型結構 ClearML 如果希望監控並追蹤實驗結果，也希望多人一起共享實驗記錄，也可以使用支援AutoKeras的MLOps平台ClearML，以互動示網頁呈現。\nfrom clearml import Task\nfrom clearML import Task # register at https://app.community.clear.ml # get the \u0026#39;key\u0026#39; and \u0026#39;secret\u0026#39;： Task.set_credentials( api_host=\u0026#39;https://api.community.clear.ml\u0026#39;, web_host=\u0026#39;https://app.community.clear.ml\u0026#39;, files_host=\u0026#39;https://files.community.clear.ml\u0026#39;, key=\u0026#39;xxxxx\u0026#39;, secret=\u0026#39;xxxxxx\u0026#39; ) # Create a new ClearML task # Visit https://app.community.clear.ml/dashboard task = Task.init(project_name=\u0026#39;autokeras age\u0026#39;, task_name=\u0026#39;age regressor\u0026#39;) ClearML主頁面中可查看project overview 除了訓練過程loss,accuracy變化趨勢，也包含機器的使用狀況 模型中weight,bias的變化  ClearML也會自動上傳記錄matplotlib跟seaborn儲存在其監控目錄的結果  Example 2 - Age prediction 另一個範例是年齡預測，透過照片來預測年齡，屬於迴歸問題。 Dataset: wiki_crop， 經圖像篩選以及前處理後，轉換成52984張128*128的圖片。\nTraining data 使用AutoModel可進行模型細節自定義，寫法上很像Keras函數式API(Functional API)，AutoKeras再從我們給的條件下，從中找到最佳結果。\ninput_node = ak.ImageInput() # If just want to try EfficientNet b0~b7 output_node = ak.ImageBlock(block_type=\u0026#39;efficient\u0026#39;, normalize=True, augment=True)(input_node) output_node = ak.RegressionHead(droupout=0.25)(output_node) reg = ak.AutoModel(inputs=input_node,outputs=output_node, max_trials=3) 在簡單訓練幾個epoch後的成果(Testing)  AutoKeras也支援處理Multi-modal資料跟multi-task模型的問題，利用AutoModel來自定義更複雜的模型結構。例如下圖包含兩種類型的輸入資料，而最後須同時預測分類標籤以及迴歸數值。  Source: Multi-Modal and Multi-Task\nAdditional Resources  AutoKeras Getting started with TensorBoard.dev AutoML 自動化機器學習  ","permalink":"https://bruceewue.github.io/blog/autokeras/","tags":["MLOps","AutoML"],"title":"AutoKeras+ClearML"},{"categories":["Data"],"contents":"因為想了解英國倫敦當地工程師就業的情況，利用空餘時間做了一套職缺分析系統(目前以data相關職缺為主)，包含自動排程分散式爬蟲、職缺分析儀表版，並另提供資料API。成果展示於本文末。\nProject Diagram Environment Development: Ubuntu 20.04(WSL2) + python 3.9\nDeployment: VM(2 OCPUs(ARM64), 12GB RAM)*2 in Oracle Cloud\nData source 職缺搜尋介面: Source: Indeed\n目前以英國最大的求職網Indeed為主，為避免對服務器造成負擔，只爬取位於London地區，資料科學相關的正職職缺(Ex. ML engineer/Data engineer/Data scientist/Data analyst/Deep learning\u0026hellip;)，且只截取近三個月內資料，並在每日排程中自動抓取當天新增的職缺；將來可再新增其他平台或其他類型職缺。\nWhy distributed?  增加爬蟲效率。 當爬蟲程式增多時可能單一電腦負荷不了，且如果機器突然故障會讓爬蟲程式因此中斷。 盡量防止因過度爬蟲被封鎖IP(搭配程式中放置sleep使用)。  Celery/RabbitMQ/Flower Celery是分散式的工作佇列系統，它是任務隊列的管理工具，Celery 專注於即時任務處理，支持任務調度。\n分散式爬蟲架構中有三個腳色:\n Producer，派發任務給Broker Message Broker，接收任務並轉發給Worker (本篇採用RabbitMQ:分散式任務轉發/訊息傳遞中心) Worker，從Broker中接收任務並執行(Scraper)  流程: producer 發出任務訊息到 queue 中排隊，workers們(Celery提供的任務執行的單元)連線到broker後，可根據設定紛紛到 queue 去接收任務來進行處理。\n下圖為RabbitMQ的Overview介面，可以看到目前queue中有兩個任務等待worker處理，在queue的頁面中也可以清楚了解每個queue的狀態以及任務的資訊。 另外可以透過透過Flower來監控worker的狀態，看是否在線以及任務執行狀況 本專案目前只有開啟兩個worker，可根據需求增減。 worker執行的task清單 Scheduling 因為需要每日自動爬取當日的新職缺，這裡透過python套件APScheduler來協助執行，以下程式片段為設定一個定時器，在週一至週日每天台北時間12:00時執行sent_crawler_task_DE這個function\nscheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Taipei\u0026#34;) scheduler.add_job( id=\u0026#34;sent_crawler_task_DE\u0026#34;, func=sent_crawler_task_DE, trigger=\u0026#34;cron\u0026#34;, hour=\u0026#34;12\u0026#34;, minute=\u0026#34;0\u0026#34;, day_of_week=\u0026#34;*\u0026#34;, ) API \u0026amp; Load test 透過FastAPI創建RESTful API，FastAPI可自動生成OpenAPI文件(OAS3)，可直接在Web上對API發送request。\n輸入position以及location兩個參數後，會回傳json格式的職缺資料 可以試著對建立好的API進行壓力測試，這部分可使用python套件ApacheBench(簡稱ab)，模擬多個使用者同時對服務器發送多個request 透過以下指令對部署在本機的API服務進行壓力測試 (concurrency=10, requests=1000)\napt-get install apache2-utils -y\rab -c 10 -n 1000 'http://127.0.0.1:8888/'\r結果如下 Request per second: 每秒能承受多少request Time per request: 平均每個request所花費的時間(ms) Failed requests: 代表有幾個request失敗，和服務器的穩定度有關\nDashboard 為了能清楚的了解職缺狀況，採用Redash這套開源BI系統製作儀表板，好處是可方便的透過撰寫SQL Query直接從DB抓取資料，再將成果圖像化呈現(目前支援Chart,tunnel,map,pivot table,word cloud\u0026hellip;等等)，成果如下。\n分別針對每日張貼的各職缺數量、各職缺平均薪資、JD的文字雲來進行分析 Deployment 由於此應用需啟動不少service，面臨到多個docker-compose(API,scraper,rabbitmq,mysql,redash\u0026hellip;)，需要有工具可以統一管理，此處使用Docker Swarm，透過Manager與Worker的架構概念管理多台機器(也可使用K8s)，可透過Manager來統一管理全部服務，包含更新服務、部署服務、查看LOG等等都變得相當方便；並搭配UI介面portainer進行管理監控。\n由下圖中可以看到目前集群中有兩台機器(node) 可以在單個介面內統一管理所有服務 DDNS \u0026amp; SSL 如果要讓API能上線提供服務，那會需要能提供一個網址而非IP，一般網域是需要收費的，可以到domcomp輸入有興趣的網域並查看價錢與是否已被買走；而本專案使用的是No-IP提供的免費DDNS服務，每月需手動更新效期。\nSSL則為安全性憑證，提供瀏覽器或電腦和伺服器之間建立加密連，成功安裝後瀏覽器網址旁會出現HTTPS圖案，簽發SSL憑證的機構很多，此處採用Let\u0026rsquo;s Encrypt發送的免費SSL憑證，透過Traefik這個工具進行反向代理，包括更好的管理所有服務的網址、DNS、Load balance、並自動更新SSL憑證，且易於跟Docker結合使用。\nSource:Traefik\n下圖為traefik v2的dashboard\nCI/CD 在專案迭代的過程中，可以透過CICD工具使得測試與部署流程自動化以降低出錯的機會；本篇使用Gitlab CI/CD，透過Specific Runner來協助執行CICD指令，目前設定當merge request時才觸發CI，自動化測試並建立docker image以及上傳docker hub，並在下tag後才觸發deploy，部署服務到docker swarm中。類似的工具有CircleCI、Github Actio等，Gitlab的好處是開源可以免費部署在自家服務器(on-premises)。\n以API repository為例，流程為test-\u0026gt;build-\u0026gt;deploy CI過程中測試覆蓋率(Test Coverage) Monitoring 部署服務之後還需要進行監控，包含各服務的運作狀態以及機器的運行狀態是否符合預期；透過Prometheus串接traefik與node-exporter抓取時序數據，並將結果可視化呈現於Grafana儀表板中\nSource:Traefik2.2\n目前使用兩台機器，Node數為2 Source:Node Exporter for Prometheus Dashboard EN\nResult  Data API  https://bit.ly/3gGBTUX\n Dashboard  https://bit.ly/3oQywix \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rAdditional Resources  Python 大數據專案 X 工程 X 產品 資料工程師的升級攻略  ","permalink":"https://bruceewue.github.io/blog/job_scrape/","tags":["Data engineering","Python","Dashboard"],"title":"Job Scraping and Analysis"},{"categories":["Machine Learning"],"contents":"在學習了眾多機器學習、資料分析等方法後不禁思考:下一步呢? 該如何讓建立的模型或分析的成果能實際落地提供使用?\n本篇以天池競賽中的製造業數據為例，創建機器學習系統，包含模型建置、API服務部署及系統監控，並結合CICD將流程自動化。\nData source 本數據集是關於化工連續製程的數據，目標是希望預測Yield，屬於迴歸問題(regression)，本案例中模擬從AWS S3取得資料\nDigital Manufacturing Algorithm Competition of JinNan Tianjin\nModel Deployment 要建立一個機器學習系統，ML code可能只佔相當小的一部份，大多數會是在處理處理資料蒐集、特徵工程、部署、系統監控等等 Source:Hidden Technical Debt in Machine Learning Systems(2015)\n以機器學習建模來說，一般會經過Data validation-\u0026gt;Feature engineering-\u0026gt;Model training-\u0026gt;Model deployment。機器學習系統比起傳統IT系統多了Data+Model，以致較難以明確規範系統行為；其中要注意的是\u0026rdquo;Reproducibility\u0026rdquo;, 意思是在上述各個流程模組中，必須確保當給定一樣的Input就應該得到一樣的Output，這是ML系統所面臨的重要挑戰；另外也是必須針對系統撰寫測試(Testing)的重要原因，期望能降低系統發生非預期行為(Uncertainty)的風險。\n那該如何確保\u0026rdquo;Reproducibility\u0026rdquo;? 基本有三個方向\n 版本控制 (Version Control) 盡可能設置Seed (if require randomness) 使用容器 (Container)  End-to-end ML system 下圖為建構端到端ML系統的流程，並且可能會不斷地迭代。 Source: Continuous Delivery for Machine Learning end-to-end process\n下圖是在學習機器學習部署的過程所歸納整理，本案例也參考這樣的設計來構建系統 以下針對幾點說明\nProductionize Model 其實我們要部署的並非只有模型本身，而是整個從資料處理到建模部署的pipeline。在這部分會將特徵工程、建模等一系列步驟透過sklearn.pipeline進行封裝，將訓練完的模型(.pkl)給予版本並將前處理、模型訓練、設定檔等打包成package並推送至Python package index server,常見的有公開的PyPI,而此處是使用gemfury,為的是能夠建立不公開的Package Repository\nDeployment(Sering the model via API) 在部署的部分，此處採用Python 3.6+之支援異步高效能框架FastAPI，好處是可以自動生成openAPI sepc並且可以簡單的直接使用Pydantic進行Schema Validation。\nSource: tiangolo/fastapi\n下圖是本例的API docs 在應用的部署上會透過Docker將服務容器化，由於需要運作多個服務(MLAPI, Prometheus, Grafana, cAdvisor)，此處會透過docker-compose一次啟動\nWhy docker for ML?  重現性(Reproducibility): 以確保每次運行結果一致 可擴展性(Scalibility): 各大雲端皆支援，易於整合 隔離性(Isolation): 確保服務獨立運作且資源隔離(Process Isolation) 維護性(Maintainability): 環境易設定且可攜帶使團隊間容易分享 Source: SUPERCHARGE YOUR DEVELOPMENT ENVIRONMENT (USING DOCKER)  Deploy to Cloud 可以將應用部署上雲端以提供服務，可以選擇是PaaS或是IaaS，此處各選擇Heroku 以及 AWS 來部署，PaaS 或是 IaaS的差異如下圖，選擇IaaS的話有較多部份需要自己管理，但也相對較為彈性。 Source: RedHat\n此處以AWS部署為例，使用到的服務有\n  Elastic Container Registry(ECR): 將打包好的image上傳到此存放，供其他AWS服務存取   Amazon Elastic Container Service(ECS):AWS上管理Dokcer container的服務，下圖為ECS的架構，需先創建一個Cluster在定義內部的Service與Task\nSource: AWS\n  下圖為本案例實作時所創建的Cluster，並且創建了一個名為custom-service的服務 Monitoring 透過Prometheus與Grafana的搭配來搭建監控儀表板。下圖為Prometheus架構，透過HTTP的PULL方式週期性抓取並儲存時間序列資料，只要提供對應的HTTP介面並且符合 Promethues 定義的格式即可監控；且有提供PromQL語言進行查詢再透過Grafana進行資料可視化，當異常發生時也可以設置alert警報。\nSource: Prometheus\n本案例在監控的部分主要分三個部分:\n Docker Monitoring: 用於監控Docker系統資源，使用工具:google/cAdvisor Source: 8bitmen.com  下圖為案例示範，主要針對四個服務的CPU跟記憶體進行監控 Model Monitoring: 可以針對模型的預測值進行監控  每單位時間有多少預測值，是否發生異常 監控模型的預測值在不同時間點是否發生顯著變化(可搭配Z-score/SEM/STD等統計資訊)   Log  可以用ELK Stack進行    如果有text類型資料，Kibana相對Grafana適合，時間序列DB可能因high cardinality造成performance的問題\nContinuous Integration/Continuous Deployment(CI/CD) 主要是希望能將應用開發測試與部署的每個階段自動化，可讓系統是在一個\u0026quot;always releasable\u0026quot;的狀態，且整個過程標準化、透明化並易於追蹤。CI/CD的工具相當多，此處使用的工具是CircleCI Source:CircleCI\n首先須撰寫設定檔config.yml，透過定義各種job跟steps來定義流程中的每個步驟，包含所需的環境或設定，所要執行的指令等等。CircleCI可以方便地與github專案連動，當新開一個Pull request，或後續該分支的commit時，就會觸發jobs。\n下圖為本案範例，當test_app(mlapi的單元測試與整合測試)通過後，才進行應用的部署，並且只有在merger到main並給予該commit tag後才進行model package的上傳發佈。\n下圖以\u0026quot;test_app\u0026quot;這個job為例，其中tox是一款python自動化測試工具，可以整合pytest一起使用。 Additional Resources  Deployment of Machine Learning Models - Online Course Testing and Monitoring Machine Learning Model Deployments - Online Course  ","permalink":"https://bruceewue.github.io/blog/ml_deployment/","tags":["MLOps","Python"],"title":"ML Model Deployment"},{"categories":["Data"],"contents":"自己來動手製作的COVID-19 Dashboard吧，本篇將透過GCP雲端服務自動抓取最新的疫情資料，並使用Google的可視化工具Data studio串接來製作每日自動更新的疫情儀表板。\nData source CSSEGISandData/COVID-19\nDiagram 透過Cloud Scheduler定時觸發Cloud Functions中部署的爬蟲程式,並將資料儲存至Cloud Storage，並由Data Studio即時讀取最新資料並製作COVID-19即時資訊儀表板。\nGCP Source: Google\n這次採用的雲端平台GCP內含相當多的服務可以使用，目前有提供300美金3個月的免費額度可使用；而本篇使用到的服務主要有Cloud Storage, Cloud Functions, Cloud Scheduler\nCloud Functions 利用Cloud Functions來新增一個可透過Http request觸發的爬蟲程式，記得在requirements.txt 寫入需安裝的套件(此處為pandas以及google-cloud-storage)，\n爬取確診人數的部分如下，也可以新增迴圈連同死亡、復原的人數一起抓取。\nimport pandas as pd def crawler(request): # get data from github df = pd.read_csv(\u0026#34;https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\u0026#34;) # wide format to long format df = pd.melt(df, id_vars=df.columns[:4], value_vars=df.columns[4:], var_name=\u0026#34;date\u0026#34;, value_name=\u0026#34;count\u0026#34;) return df.to_csv(index=False) 由於需要抓取自動後上傳至Cloud Storage，所以還需加入以下網址內程式碼 Uploading objects\nNote:\n Cloud Functions只允許在 /tmp路徑下才能存檔。 Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Storage 雲端儲存空間，新增bucket後即可開始使用，也能像google drive一樣手動上傳檔案，但Cloud Storage的優勢是更能方便地在GCP上與各服務串接；免費的部分有5GB的額度。\n觸發Cloud functions後可以確認看看是否有確實上傳確診、死亡、復原的資料 Note:\n Cloud functions 與 Cloud Storage建議在同個地區，避免額外傳輸費用。  Cloud Scheduler 可以依據原始資料的更新時間來設定希望多久執行一次爬蟲，設定方式是像在Linux中的Crontab排程，使用Cron語法，可以利用這個網站確認自己撰寫的排程是否符合預期。\n設定完成的狀態如下 Note:\n Cloud Scheduler完全免費方案只有設定3個Job的額度  Data Studio 在設定好資料後就可以嘗試來把它視覺化了，這裡採用的是google免費的Data Studio，算是滿容易上手的工具，跟GCP串接當然也是沒有問題的。\n此處新增資料源-\u0026gt;從Data Storage內選取三份csv檔案\nResult COVID-19 Dashboard Animated Bar Chart  即時資訊互動儀表板如下，可自由選擇Country/Region (建議可點擊右下角\u0026quot;Google數據分析\u0026quot;觀看完整報表)  \r.iframe-container { position: relative;\rwidth: 100%;\rheight: 0;\rpadding-bottom: 56%; } .iframe-container iframe { position: absolute;\rwidth: 100%;\rheight: 100%;\rleft: 0;\rtop: 0;\r}\r\r\r\rAdditional Resources  打造動態報表！雲端 Python 爬蟲資料流  ","permalink":"https://bruceewue.github.io/blog/covid_dashboard/","tags":["Data studio","Dashboard","GCP","Python"],"title":"Create COVID-19 Dashboard by Data studio"},{"categories":["Others"],"contents":"Markdown Example\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\n  Steve Francia test\n  List  有序1  四個空格內縮   有序2 有序3  Unordered List  無序1  四個空格內縮   無序2  Blockquote  This is a blockquote example.\n Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) print(\u0026#34;hello Bruce\u0026#34;) No language indicated, so no syntax highlighting. But let's throw in a \u0026lt;b\u0026gt;tag\u0026lt;/b\u0026gt;.\rInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nTables Colons can be used to align columns.\n   Tables Are Cool     col 3 is right-aligned $1600   col 2 is centered $12   zebra stripes are neat $1    There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\n   Markdown Less Pretty     Still renders nicely   1 2 3    Youtube embeded   ","permalink":"https://bruceewue.github.io/blog/markdown_test/","tags":["Hugo","Markdown"],"title":"Markdown test"}]